{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "635f63ea",
   "metadata": {
    "collapsed": true,
    "id": "635f63ea",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "!pip install lime\n",
    "!pip install git+https://github.com/jacobgil/pytorch-grad-cam.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e1ef1b",
   "metadata": {
    "id": "74e1ef1b"
   },
   "outputs": [],
   "source": [
    "# libraries:\n",
    "import copy\n",
    "import itertools\n",
    "import os\n",
    "import random\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# Deep Learning - PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "\n",
    "# Computer Vision & Image Processing\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from skimage import segmentation\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.segmentation import felzenszwalb\n",
    "from skimage.transform import resize\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine Learning Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Explainability Frameworks\n",
    "import lime\n",
    "from lime import lime_image\n",
    "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
    "import shap\n",
    "from pytorch_grad_cam import GradCAM\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "\n",
    "# Suppress Warnings\n",
    "warnings.filterwarnings('ignore', message='unrecognized nn.Module: Flatten')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tGgebZgx6v9W",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tGgebZgx6v9W",
    "outputId": "2b037f43-3fcd-48dd-ec10-d70c13997417"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K-QxaR9d6yRT",
   "metadata": {
    "id": "K-QxaR9d6yRT"
   },
   "source": [
    "DATA + preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YLG7WpTX6x5N",
   "metadata": {
    "id": "YLG7WpTX6x5N"
   },
   "outputs": [],
   "source": [
    "train_dir = 'drive/MyDrive/MT/archive/Training'\n",
    "test_dir = 'drive/MyDrive/MT/archive/Testing'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2S9zD0DU66Ks",
   "metadata": {
    "id": "2S9zD0DU66Ks"
   },
   "outputs": [],
   "source": [
    "#transformations:\n",
    "train_transform = transforms.Compose([\n",
    "    #transforms.TrivialAugmentWide(),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.1848, 0.1848, 0.1848],\n",
    "                           std=[0.1768, 0.1768, 0.1768])\n",
    "])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.1848, 0.1848, 0.1848],\n",
    "                           std=[0.1768, 0.1768, 0.1768])\n",
    "])\n",
    "\n",
    "#for visualization:\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9PZPD21X68KZ",
   "metadata": {
    "id": "9PZPD21X68KZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
    "full_dataset = datasets.ImageFolder(train_dir, transform=val_test_transform)\n",
    "\n",
    "test_dataset_norm = datasets.ImageFolder(test_dir, transform=val_test_transform)\n",
    "test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Hk9Pv3HS8U4V",
   "metadata": {
    "id": "Hk9Pv3HS8U4V"
   },
   "outputs": [],
   "source": [
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "test_loader_norm = DataLoader(test_dataset_norm, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yUKVKOGu7S0t",
   "metadata": {
    "id": "yUKVKOGu7S0t"
   },
   "source": [
    "class-weights:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vtkQAsgC7Qzg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vtkQAsgC7Qzg",
    "outputId": "9cecc14d-b75e-404d-f5e8-d038ed6605fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: {0: 4.323996971990916, 1: 4.265870052277819, 2: 3.581191222570533, 3: 3.9203843514070007}\n"
     ]
    }
   ],
   "source": [
    "#Class weights: {0: 4.323996971990916, 1: 4.265870052277819, 2: 3.581191222570533, 3: 3.9203843514070007}\n",
    "\n",
    "train_counts = {\n",
    "    0: 1321,  # Glioma\n",
    "    1: 1339,  # Meningioma\n",
    "    2: 1595,  # No Tumor\n",
    "    3: 1457   # Pituitary\n",
    "}\n",
    "\n",
    "total_samples = sum(train_counts.values())  # 5712\n",
    "class_weights = {class_idx: total_samples / count\n",
    "                for class_idx, count in train_counts.items()}\n",
    "\n",
    "print(\"Class weights:\", class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iEQQQbG87cV5",
   "metadata": {
    "id": "iEQQQbG87cV5"
   },
   "source": [
    "MODEL:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tE5JYu8K7am3",
   "metadata": {
    "id": "tE5JYu8K7am3"
   },
   "outputs": [],
   "source": [
    "class BrainCNN(nn.Module):\n",
    "    def __init__(self, num_classes=4):\n",
    "        super(BrainCNN, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.Dropout2d(0.2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.Dropout2d(0.2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 16 * 16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def train_and_validate(model, train_loader, val_loader, test_loader, class_weights=None, epochs=10):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "\n",
    "    # Calculate class weights if not provided\n",
    "    if class_weights is None:\n",
    "        class_weights = class_weights\n",
    "\n",
    "    # Convert class weights to tensor\n",
    "    weight_tensor = torch.FloatTensor([class_weights[i] for i in range(len(class_weights))]).to(device)\n",
    "\n",
    "    # Initialize weighted loss function\n",
    "    criterion = nn.CrossEntropyLoss(weight=weight_tensor)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'train_acc': [],\n",
    "        'val_loss': [],\n",
    "        'val_acc': []\n",
    "    }\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            train_total += labels.size(0)\n",
    "            train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_train_loss = train_loss/len(train_loader)\n",
    "        epoch_train_acc = 100 * train_correct / train_total\n",
    "\n",
    "        \n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "\n",
    "                val_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                val_total += labels.size(0)\n",
    "                val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = val_loss/len(val_loader)\n",
    "        epoch_val_acc = 100 * val_correct / val_total\n",
    "\n",
    "        history['train_loss'].append(epoch_train_loss)\n",
    "        history['train_acc'].append(epoch_train_acc)\n",
    "        history['val_loss'].append(epoch_val_loss)\n",
    "        history['val_acc'].append(epoch_val_acc)\n",
    "\n",
    "        print(f'Epoch {epoch+1}:')\n",
    "        print(f'Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_acc:.2f}%')\n",
    "        print(f'Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {epoch_val_acc:.2f}%')\n",
    "\n",
    "   \n",
    "    model.eval()\n",
    "    test_correct = 0\n",
    "    test_total = 0\n",
    "    test_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            test_total += labels.size(0)\n",
    "            test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('\\nFinal Test Results:')\n",
    "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
    "    print(f'Test Accuracy: {100 * test_correct / test_total:.2f}%')\n",
    "\n",
    "    return history, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9PhOxSiU7fOS",
   "metadata": {
    "id": "9PhOxSiU7fOS"
   },
   "source": [
    "load models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KCuAyRUh7iM7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KCuAyRUh7iM7",
    "outputId": "19abd1f9-025b-4095-c1f2-c3e6def6f3a2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-9-e1b7da3716d2>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded base model\n",
      "Successfully loaded trivial_aug model\n",
      "Successfully loaded gans model\n",
      "Successfully loaded combined model\n"
     ]
    }
   ],
   "source": [
    "def load_model_from_checkpoint(checkpoint_path, model_class):\n",
    "    \n",
    "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
    "    model = model_class()\n",
    "\n",
    "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        history = checkpoint.get('history', None)\n",
    "    else:\n",
    "        model.load_state_dict(checkpoint)\n",
    "        history = None\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    return model, history\n",
    "\n",
    "\n",
    "model_paths = {\n",
    "    'base': 'drive/MyDrive/results/archive21/base_model.pth',\n",
    "    'trivial_aug': 'drive/MyDrive/results/archive21/TA_model_3.pth',\n",
    "    'gans': 'drive/MyDrive/results/archive21/gan_model_2.pth',\n",
    "    'combined': 'drive/MyDrive/results/archive21/combined_model_2.pth'\n",
    "}\n",
    "\n",
    "models = {}\n",
    "histories = {}\n",
    "\n",
    "for model_name, path in model_paths.items():\n",
    "    try:\n",
    "        model, history = load_model_from_checkpoint(path, BrainCNN)\n",
    "        models[model_name] = model\n",
    "        histories[model_name] = history\n",
    "        print(f\"Successfully loaded {model_name} model\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {model_name} model: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "agapWZ7g7j5_",
   "metadata": {
    "id": "agapWZ7g7j5_"
   },
   "outputs": [],
   "source": [
    "base_model = models['base']\n",
    "ta_model = models['trivial_aug']\n",
    "gan_model = models['gans']\n",
    "combined_model = models['combined']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "jzHeWgshGcSV",
   "metadata": {
    "id": "jzHeWgshGcSV"
   },
   "source": [
    "see misclassified :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eQSdI4QoJ9mH",
   "metadata": {
    "id": "eQSdI4QoJ9mH"
   },
   "outputs": [],
   "source": [
    "def get_misclassified_indices(model, test_loader):\n",
    "    misclassified = []\n",
    "    base_idx = 0\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (data, targets) in enumerate(test_loader):\n",
    "            outputs = model(data.to(next(model.parameters()).device))\n",
    "            predictions = outputs.argmax(dim=1)\n",
    "            incorrect = predictions.cpu() != targets\n",
    "            batch_indices = incorrect.nonzero().squeeze().tolist()\n",
    "\n",
    "            if isinstance(batch_indices, int):\n",
    "                batch_indices = [batch_indices]\n",
    "\n",
    "            dataset_indices = [base_idx + i for i in batch_indices]\n",
    "            misclassified.extend(dataset_indices)\n",
    "\n",
    "            base_idx += len(data)\n",
    "\n",
    "    return misclassified\n",
    "\n",
    "misclassified_indices = {\n",
    "    model_name: get_misclassified_indices(model, test_loader_norm)\n",
    "    for model_name, model in models_dict.items()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kQD_tUsJKKR7",
   "metadata": {
    "collapsed": true,
    "id": "kQD_tUsJKKR7",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "misclassified_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3yihh0wgGRcp",
   "metadata": {
    "id": "3yihh0wgGRcp"
   },
   "source": [
    "# <a id='gen'>Fidelity: LIME</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "nFpEi45XCi9X",
   "metadata": {
    "id": "nFpEi45XCi9X"
   },
   "outputs": [],
   "source": [
    "def normalize_heatmap(heatmap):\n",
    "    \n",
    "    if isinstance(heatmap, torch.Tensor): #convert tensor to numpy if needed\n",
    "        heatmap = heatmap.cpu().numpy()\n",
    "\n",
    "    # Ensure heatmap is 2D:\n",
    "    if len(heatmap.shape) > 2:\n",
    "        # Check if channels are last dimension:\n",
    "        if heatmap.shape[-1] in [3, 4]:\n",
    "            heatmap = np.mean(heatmap, axis=-1)\n",
    "        # Check if channels are first dimension:\n",
    "        elif heatmap.shape[0] in [3, 4]:\n",
    "            heatmap = np.mean(heatmap, axis=0)\n",
    "\n",
    "    #normalize to [0,1]\n",
    "    if heatmap.max() - heatmap.min() != 0:\n",
    "        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
    "\n",
    "    return heatmap\n",
    "\n",
    "def prepare_image(image, denormalize=True):\n",
    "    if torch.is_tensor(image):\n",
    "        image = image.cpu().numpy()\n",
    "\n",
    "    if image.shape[0] == 3:  # CHW to HWC\n",
    "        image = np.transpose(image, (1, 2, 0))\n",
    "\n",
    "    if denormalize:\n",
    "        mean = np.array([0.1948, 0.1948, 0.1948])\n",
    "        std = np.array([0.1768, 0.1768, 0.1768])\n",
    "        image = image * std + mean\n",
    "\n",
    "    return np.clip(image, 0, 1)\n",
    "\n",
    "\n",
    "def get_specific_batch_images(loader, index):\n",
    "    \n",
    "    for i, (images, labels) in enumerate(loader): #get specific img from dataloader\n",
    "        if i * loader.batch_size <= index < (i + 1) * loader.batch_size:\n",
    "            batch_idx = index - (i * loader.batch_size)\n",
    "            return images[batch_idx], labels[batch_idx]\n",
    "    return None, None\n",
    "\n",
    "def generate_lime_explanation(model, input_image, class_names, num_samples=1000): #lime for single img.\n",
    "    def batch_predict(images):\n",
    "        model.eval()\n",
    "        batch = torch.stack([torch.from_numpy(i).permute(2, 0, 1).float() for i in images], dim=0)\n",
    "        device = next(model.parameters()).device\n",
    "        batch = batch.to(device)\n",
    "        with torch.no_grad():\n",
    "            logits = model(batch)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "        return probs.cpu().numpy()\n",
    "\n",
    "    if torch.is_tensor(input_image):\n",
    "        input_image = prepare_image(input_image, denormalize=True)\n",
    "\n",
    "#actual predictions:\n",
    "    with torch.no_grad():\n",
    "        device = next(model.parameters()).device\n",
    "        input_tensor = torch.from_numpy(input_image.transpose(2, 0, 1)).float().unsqueeze(0).to(device)\n",
    "        logits = model(input_tensor)\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        pred_class = torch.argmax(probs[0]).item()\n",
    "        confidence = probs[0][pred_class].item() * 100\n",
    "\n",
    "    explainer = lime_image.LimeImageExplainer()\n",
    "    segmenter = SegmentationAlgorithm('felzenszwalb', scale=100, sigma=0.8, min_size=50)\n",
    "\n",
    "    explanation = explainer.explain_instance(\n",
    "        input_image,\n",
    "        batch_predict,\n",
    "        labels= len(class_names),\n",
    "        hide_color=0,\n",
    "        num_samples=num_samples,\n",
    "        segmentation_fn=segmenter\n",
    "    )\n",
    "\n",
    "    exp_img, mask = explanation.get_image_and_mask(\n",
    "        pred_class,\n",
    "        positive_only=False,\n",
    "        num_features=20,\n",
    "        hide_rest=False,\n",
    "        min_weight=0.01\n",
    "    )\n",
    "\n",
    "    mask = normalize_heatmap(mask)\n",
    "\n",
    "    return exp_img, mask, pred_class, confidence\n",
    "\n",
    "def visualize_results(results, original_image, class_names):\n",
    "    \n",
    "    fig = plt.figure(figsize=(20, 4))\n",
    "    fig.suptitle('LIME Explanations Comparison', fontsize=16)\n",
    "\n",
    "    ax = plt.subplot(1, 5, 1)\n",
    "    orig_img = prepare_image(original_image, denormalize=True)\n",
    "    ax.imshow(orig_img)\n",
    "    ax.set_title('Original Image')\n",
    "    ax.axis('off')\n",
    "\n",
    "    for idx, (model_name, result) in enumerate(results.items(), start=2): #lime for each model!\n",
    "        pred_class = result.get('predicted_class', None)\n",
    "        pred_info = \"\"\n",
    "        if pred_class is not None and class_names is not None:\n",
    "            pred_info = f\"Pred: {class_names[pred_class]}\"\n",
    "            if 'probabilities' in result:\n",
    "                conf = result['probabilities'][pred_class] * 100\n",
    "                pred_info += f\"\\nConf: {conf:.1f}%\"\n",
    "\n",
    "        ax = plt.subplot(1, 5, idx)\n",
    "        ax.imshow(orig_img)\n",
    "        ax.imshow(result['lime'], cmap='RdYlBu_r', alpha=0.5)\n",
    "        ax.set_title(f'{model_name.capitalize()}\\n{pred_info}')\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def visualize_model_comparisons(models_dict, test_loader_norm, test_loader_unnorm, class_names, image_index=0):\n",
    "    \n",
    "    image_norm, label = get_specific_batch_images(test_loader_norm, image_index)\n",
    "    image_unnorm, _ = get_specific_batch_images(test_loader_unnorm, image_index)\n",
    "\n",
    "    direct_preds = {}\n",
    "    for model_name, model in models_dict.items():\n",
    "        with torch.no_grad():\n",
    "            device = next(model.parameters()).device\n",
    "            input_tensor = image_norm.unsqueeze(0).to(device)\n",
    "            logits = model(input_tensor)\n",
    "            probs = F.softmax(logits, dim=1)\n",
    "            pred_class = torch.argmax(probs[0]).item()\n",
    "            confidence = probs[0][pred_class].item() * 100\n",
    "            direct_preds[model_name] = {\n",
    "                'class': pred_class,\n",
    "                'confidence': confidence,\n",
    "                'probabilities': probs[0].cpu().numpy()\n",
    "            }\n",
    "\n",
    "    true_class = class_names[label]\n",
    "    print(f\"\\nTrue class: {true_class}\")\n",
    "    print(\"Model predictions:\")\n",
    "    for model_name, pred in direct_preds.items():\n",
    "        print(f\"{model_name}: {class_names[pred['class']]} ({pred['confidence']:.1f}%)\")\n",
    "\n",
    "    fig = plt.figure(figsize=(20, 8))\n",
    "    fig.suptitle(f'LIME Explanations Comparison\\nTrue Class: {true_class}', fontsize=16)\n",
    "\n",
    "    ax = plt.subplot(2, 5, 1)\n",
    "    orig_img = prepare_image(image_unnorm, denormalize=False)\n",
    "    ax.imshow(orig_img)\n",
    "    ax.set_title(f'Original Image\\nTrue: {true_class}')\n",
    "    ax.axis('off')\n",
    "\n",
    "    print(\"\\nGenerating LIME explanations:\")\n",
    "    for idx, (model_name, model) in enumerate(models_dict.items(), start=1):\n",
    "        print(f\"\\nProcessing {model_name}...\")\n",
    "\n",
    "        pred_info = direct_preds[model_name]\n",
    "        pred_class = pred_info['class']\n",
    "        confidence = pred_info['confidence']\n",
    "\n",
    "        def batch_predict(images):\n",
    "            model.eval()\n",
    "            batch = torch.stack([torch.from_numpy(i).permute(2, 0, 1).float() for i in images], dim=0)\n",
    "            device = next(model.parameters()).device\n",
    "            batch = batch.to(device)\n",
    "            with torch.no_grad():\n",
    "                logits = model(batch)\n",
    "                probs = F.softmax(logits, dim=1)\n",
    "            return probs.cpu().numpy()\n",
    "\n",
    "        input_image = prepare_image(image_norm, denormalize=True)\n",
    "        explainer = lime_image.LimeImageExplainer()\n",
    "        segmenter = SegmentationAlgorithm('felzenszwalb', scale=100, sigma=0.8, min_size=50)\n",
    "\n",
    "        explanation = explainer.explain_instance( #explanation for all classes\n",
    "            input_image,\n",
    "            batch_predict,\n",
    "            top_labels=len(class_names),\n",
    "            hide_color=0,\n",
    "            num_samples=1000,\n",
    "            segmentation_fn=segmenter\n",
    "        )\n",
    "\n",
    "        if pred_class in explanation.local_exp:\n",
    "            exp_img_pred, mask_pred = explanation.get_image_and_mask(\n",
    "                pred_class,\n",
    "                positive_only=False,\n",
    "                num_features=20,\n",
    "                hide_rest=False,\n",
    "                min_weight=0.01\n",
    "            )\n",
    "            mask_pred = normalize_heatmap(mask_pred)\n",
    "\n",
    "            ax = plt.subplot(2, 5, idx + 1)\n",
    "            ax.imshow(orig_img)\n",
    "            ax.imshow(mask_pred, cmap='RdBu', alpha=0.5)\n",
    "            ax.set_title(f'{model_name.capitalize()}\\nPredicted: {class_names[pred_class]}\\nConf: {confidence:.1f}%')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            print(f\"Warning: No explanation found for predicted class {class_names[pred_class]} in {model_name}.\")\n",
    "\n",
    "        if label in explanation.local_exp:\n",
    "            exp_img_true, mask_true = explanation.get_image_and_mask(\n",
    "                label,\n",
    "                positive_only=False,\n",
    "                num_features=20,\n",
    "                hide_rest=False,\n",
    "                min_weight=0.01\n",
    "            )\n",
    "            mask_true = normalize_heatmap(mask_true)\n",
    "\n",
    "            ax = plt.subplot(2, 5, idx + 6)\n",
    "            ax.imshow(orig_img)\n",
    "            ax.imshow(mask_true, cmap='RdBu', alpha=0.5)\n",
    "            ax.set_title(f'True Class Features\\n{true_class}')\n",
    "            ax.axis('off')\n",
    "        else:\n",
    "            print(f\"Warning: No explanation found for true class {true_class} in {model_name}.\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sxmzbqK3fbSJ",
   "metadata": {
    "collapsed": true,
    "id": "sxmzbqK3fbSJ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    'base': base_model,\n",
    "    'trivial_aug': ta_model,\n",
    "    'gans': gan_model,\n",
    "    'combined': combined_model\n",
    "}\n",
    "\n",
    "class_names = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
    "\n",
    "# Run visualization\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=400) #meningioma cases\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=415)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=425)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7h7QHPijUAlS",
   "metadata": {
    "collapsed": true,
    "id": "7h7QHPijUAlS",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run visualization\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=350) #meningioma cases\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=600)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wyru1uB8VP__",
   "metadata": {
    "collapsed": true,
    "id": "wyru1uB8VP__",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run visualization\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=192) #glioma\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=278)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=285)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fKe6ekzqPqvM",
   "metadata": {
    "collapsed": true,
    "id": "fKe6ekzqPqvM",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run visualization\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=68) #glioma\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=287)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aTOW7Otd0dBl",
   "metadata": {
    "collapsed": true,
    "id": "aTOW7Otd0dBl",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run visualization\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=968) #no tumor\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=987)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=938)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4F5p8OXETKdZ",
   "metadata": {
    "collapsed": true,
    "id": "4F5p8OXETKdZ",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run visualization\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1100) #pitu\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1200)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1195)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Niz_dL9NT3Ge",
   "metadata": {
    "collapsed": true,
    "id": "Niz_dL9NT3Ge",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Run visualization\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1211) #pitu\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1310)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1019)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "T3jKQdM0Kcx0",
   "metadata": {
    "id": "T3jKQdM0Kcx0"
   },
   "source": [
    "focus on misclassified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-Ph8200PKdie",
   "metadata": {
    "collapsed": true,
    "id": "-Ph8200PKdie",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=30)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=44)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=45)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=50)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=229)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7Lyps3WNMdoh",
   "metadata": {
    "collapsed": true,
    "id": "7Lyps3WNMdoh",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=3)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=43)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=4)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=44)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1089)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FNe0VNAXODwl",
   "metadata": {
    "id": "FNe0VNAXODwl"
   },
   "source": [
    "class-wise (base-focused):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FfNOgbGWOI-X",
   "metadata": {
    "collapsed": true,
    "id": "FfNOgbGWOI-X",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=702)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=635)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=701)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=714)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=843)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75HjdDzR5r4",
   "metadata": {
    "id": "c75HjdDzR5r4"
   },
   "source": [
    "miss-classified by combined:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O04oyAcDOC-m",
   "metadata": {
    "collapsed": true,
    "id": "O04oyAcDOC-m",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=159)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1043)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=224)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=356)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=548)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZeN4yosoR9kI",
   "metadata": {
    "collapsed": true,
    "id": "ZeN4yosoR9kI",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=242)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=361)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=583)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1013)\n",
    "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tIM9RbGj6tor",
   "metadata": {
    "id": "tIM9RbGj6tor"
   },
   "source": [
    "Overall fidelity scores for LIME:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71IYMsHpxUTP",
   "metadata": {
    "id": "71IYMsHpxUTP"
   },
   "source": [
    "for smaller set:\n",
    "\n",
    "        models_dict: Dictionary of models\n",
    "        test_loader_norm: Normalized test data loader\n",
    "        class_names: List of class names\n",
    "        num_samples: Number of images to evaluate per class (default reduced to 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mYgVzmW8xYTq",
   "metadata": {
    "id": "mYgVzmW8xYTq"
   },
   "outputs": [],
   "source": [
    "def calculate_class_fidelities(models_dict, test_loader_norm, class_names, num_samples=5):  #from 50 to 5\n",
    "    \n",
    "    # Initialize storage for fidelity scores\n",
    "    fidelity_scores = {model_name: {class_name: [] for class_name in class_names}\n",
    "                      for model_name in models_dict.keys()}\n",
    "\n",
    "    # Get a limited number of images and labels\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    max_images_per_class = num_samples + 2  # Add small buffer\n",
    "    class_counts = {i: 0 for i in range(len(class_names))}\n",
    "\n",
    "    for images, labels in test_loader_norm:\n",
    "        for img, label in zip(images, labels):\n",
    "            label_idx = label.item()\n",
    "            if class_counts[label_idx] < max_images_per_class:\n",
    "                all_images.append(img)\n",
    "                all_labels.append(label)\n",
    "                class_counts[label_idx] += 1\n",
    "\n",
    "        # Check if we have enough images for each class\n",
    "        if all(count >= max_images_per_class for count in class_counts.values()):\n",
    "            break\n",
    "\n",
    "    # Convert to numpy for easier indexing\n",
    "    all_labels = np.array([label.item() for label in all_labels])\n",
    "\n",
    "    # Process each class\n",
    "    for class_idx, class_name in enumerate(class_names):\n",
    "        print(f\"\\nProcessing class: {class_name}\")\n",
    "\n",
    "        # Get indices for this class\n",
    "        class_indices = np.where(all_labels == class_idx)[0]\n",
    "\n",
    "        # Sample images for this class\n",
    "        selected_indices = np.random.choice(class_indices,\n",
    "                                          size=min(num_samples, len(class_indices)),\n",
    "                                          replace=False)\n",
    "\n",
    "        # Process each selected image\n",
    "        for img_idx in selected_indices:\n",
    "            image = all_images[img_idx]\n",
    "\n",
    "            # Process each model\n",
    "            for model_name, model in models_dict.items():\n",
    "                print(f\"Processing model {model_name}, image {img_idx}\")\n",
    "\n",
    "                def batch_predict(images):\n",
    "                    model.eval()\n",
    "                    batch = torch.stack([torch.from_numpy(i).permute(2, 0, 1).float() for i in images], dim=0)\n",
    "                    device = next(model.parameters()).device\n",
    "                    batch = batch.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        logits = model(batch)\n",
    "                        probs = F.softmax(logits, dim=1)\n",
    "                    return probs.cpu().numpy()\n",
    "\n",
    "                input_image = prepare_image(image, denormalize=True)\n",
    "\n",
    "                # Get model's prediction\n",
    "                with torch.no_grad():\n",
    "                    device = next(model.parameters()).device\n",
    "                    input_tensor = image.unsqueeze(0).to(device)\n",
    "                    logits = model(input_tensor)\n",
    "                    probs = F.softmax(logits, dim=1)\n",
    "                    pred_class = torch.argmax(probs[0]).item()\n",
    "\n",
    "                # Generate LIME explanation\n",
    "                explainer = lime_image.LimeImageExplainer()\n",
    "                segmenter = SegmentationAlgorithm('felzenszwalb', scale=100, sigma=0.8, min_size=50)\n",
    "\n",
    "                explanation = explainer.explain_instance(\n",
    "                    input_image,\n",
    "                    batch_predict,\n",
    "                    top_labels=len(class_names),  # change: to explain all classes\n",
    "                    hide_color=0,\n",
    "                    num_samples=1000,\n",
    "                    segmentation_fn=segmenter\n",
    "                )\n",
    "\n",
    "                # Store fidelity score\n",
    "                fidelity_scores[model_name][class_name].append(explanation.score)\n",
    "\n",
    "    #calculate and display average fidelity scores:\n",
    "    print(\"\\nAverage Fidelity Scores per Class:\")\n",
    "    for model_name in models_dict.keys():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for class_name in class_names:\n",
    "            scores = fidelity_scores[model_name][class_name]\n",
    "            avg_score = np.mean(scores) if scores else 0\n",
    "            std_score = np.std(scores) if scores else 0\n",
    "            print(f\"{class_name}: {avg_score:.3f} ± {std_score:.3f}\")\n",
    "\n",
    "    return fidelity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1vs99FV1xTwA",
   "metadata": {
    "collapsed": true,
    "id": "1vs99FV1xTwA",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fidelity_scores = calculate_class_fidelities(\n",
    "    models_dict=models_dict,\n",
    "    test_loader_norm=test_loader_norm,\n",
    "    class_names=class_names,\n",
    "    num_samples=5 #perclass\n",
    ")\n",
    "#can save:\n",
    "#import json\n",
    "#with open('fidelity_scores.json', 'w') as f:\n",
    " #   json.dump({k: {c: list(map(float, v)) for c, v in v.items()}\n",
    "  #            for k, v in fidelity_scores.items()}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "kSZPSoBiIbe1",
   "metadata": {
    "id": "kSZPSoBiIbe1"
   },
   "source": [
    "larger set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "L-3LOXItIYmb",
   "metadata": {
    "collapsed": true,
    "id": "L-3LOXItIYmb",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fidelity_scores = calculate_class_fidelities(\n",
    "    models_dict=models_dict,\n",
    "    test_loader_norm=test_loader_norm,\n",
    "    class_names=class_names,\n",
    "    num_samples=50  #for larger\n",
    ")\n",
    "\n",
    "# Save the scores\n",
    "import json\n",
    "with open('fidelity_scores.json', 'w') as f:\n",
    "    json.dump({k: {c: list(map(float, v)) for c, v in v.items()}\n",
    "              for k, v in fidelity_scores.items()}, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "z7d-r6RZx3ni",
   "metadata": {
    "id": "z7d-r6RZx3ni"
   },
   "source": [
    "# <a id='gen'>Sanity checks (Grad-CAM)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kpkuUwyhyNrG",
   "metadata": {
    "id": "kpkuUwyhyNrG"
   },
   "outputs": [],
   "source": [
    "\n",
    "class SaveFeatures():\n",
    "    def __init__(self, module):\n",
    "        self.features = None\n",
    "        self.gradient = None\n",
    "        self.hook = module.register_forward_hook(self.hook_fn)\n",
    "\n",
    "    def hook_fn(self, module, input, output):\n",
    "        self.features = output\n",
    "\n",
    "    def remove(self):\n",
    "        self.hook.remove()\n",
    "\n",
    "def get_last_conv_layer(model):\n",
    "    last_conv_layer = None\n",
    "    for module in model.features:\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            last_conv_layer = module\n",
    "    return last_conv_layer\n",
    "\n",
    "\n",
    "def generate_gradcam(model, image, target_class=None):\n",
    "    model.eval()\n",
    "    features = None\n",
    "\n",
    "    def save_features(module, input, output):\n",
    "        nonlocal features\n",
    "        features = output\n",
    "        features.retain_grad()\n",
    "\n",
    "    # Register hook on last conv layer\n",
    "    for module in reversed(model.features):\n",
    "        if isinstance(module, torch.nn.Conv2d):\n",
    "            handle = module.register_forward_hook(save_features)\n",
    "            break\n",
    "\n",
    "    # Forward pass\n",
    "    output = model(image.unsqueeze(0))\n",
    "\n",
    "    if target_class is None:\n",
    "        target_class = output.argmax(dim=1).item()\n",
    "\n",
    "    # Backward pass\n",
    "    model.zero_grad()\n",
    "    output[0, target_class].backward()\n",
    "\n",
    "    # Calculate Grad-CAM\n",
    "    pooled_grads = torch.mean(features.grad, dim=[0, 2, 3])\n",
    "    for i in range(features.shape[1]):\n",
    "        features[:, i, :, :] *= pooled_grads[i]\n",
    "\n",
    "    heatmap = torch.mean(features, dim=1).squeeze()\n",
    "    heatmap = F.relu(heatmap)\n",
    "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() + 1e-8)\n",
    "\n",
    "    handle.remove()\n",
    "    return heatmap\n",
    "\n",
    "def compare_models_gradcam(models, image, class_names=None):\n",
    "    num_models = len(models)\n",
    "    plt.figure(figsize=(5*num_models, 5))\n",
    "\n",
    "    for idx, (name, model) in enumerate(models.items(), 1):\n",
    "        plt.subplot(1, num_models, idx)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            pred = model(image.unsqueeze(0)).argmax().item()\n",
    "\n",
    "        heatmap = generate_gradcam(model, image).detach().cpu().numpy()\n",
    "        image_np = image.permute(1, 2, 0).cpu().numpy()\n",
    "        image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())\n",
    "\n",
    "        heatmap = transforms.functional.resize(\n",
    "            torch.from_numpy(heatmap).unsqueeze(0).unsqueeze(0),\n",
    "            image_np.shape[:2]\n",
    "        ).squeeze().numpy()\n",
    "\n",
    "        plt.imshow(image_np)\n",
    "        plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "        title = f\"{name}\\n\"\n",
    "        if class_names:\n",
    "            title += f\"Pred: {class_names[pred]}\"\n",
    "        plt.title(title)\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "models_dict = {\n",
    "    'base': base_model,\n",
    "    'trivial_aug': ta_model,\n",
    "    'gans': gan_model,\n",
    "    'combined': combined_model\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CjT_aYHRkpQX",
   "metadata": {
    "collapsed": true,
    "id": "CjT_aYHRkpQX",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_loader_norm))\n",
    "image = images[25] \n",
    "\n",
    "compare_models_gradcam(models_dict, image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gMpqyOxNk2QX",
   "metadata": {
    "id": "gMpqyOxNk2QX"
   },
   "source": [
    "with predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MG6OD4X-lo3s",
   "metadata": {
    "id": "MG6OD4X-lo3s"
   },
   "outputs": [],
   "source": [
    "\n",
    "class_names = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
    "\n",
    "def show_gradcam_with_predictions(models_dict, dataset, index=0, device='cuda'):\n",
    "    \n",
    "    single_item_subset = Subset(dataset, [index])\n",
    "    single_loader = DataLoader(single_item_subset, batch_size=1, shuffle=False)\n",
    "    image, label = next(iter(single_loader))\n",
    "    image = image[0].to(device)  \n",
    "    true_label = label[0].item()\n",
    "\n",
    "    num_plots = len(models_dict) + 1  # +1 for original image\n",
    "    plt.figure(figsize=(5*num_plots, 5))\n",
    "\n",
    "    #original img:\n",
    "    plt.subplot(1, num_plots, 1)\n",
    "    image_np = image.cpu().permute(1, 2, 0).numpy()\n",
    "    image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())\n",
    "    plt.imshow(image_np)\n",
    "    plt.title(f\"Original Image\\nTrue Class: {class_names[true_label]}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # GradCAM for each model\n",
    "    for idx, (name, model) in enumerate(models_dict.items(), 2):\n",
    "        plt.subplot(1, num_plots, idx)\n",
    "\n",
    "        # Create a deep copy of the model for GradCAM\n",
    "        model_copy = copy.deepcopy(model)\n",
    "        model_copy.eval()\n",
    "        model_copy.to(device)\n",
    "\n",
    "        # Get prediction from original model\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            pred = model(image.unsqueeze(0))\n",
    "            pred_class = pred.argmax().item()\n",
    "            confidence = torch.softmax(pred, dim=1)[0, pred_class].item()\n",
    "\n",
    "        # Generate GradCAM using the copy\n",
    "        heatmap = generate_gradcam(model_copy, image.clone()).detach().cpu().numpy()\n",
    "\n",
    "        # Clear memory\n",
    "        del model_copy\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        # Resize heatmap to match image size\n",
    "        heatmap = transforms.functional.resize(\n",
    "            torch.from_numpy(heatmap).unsqueeze(0).unsqueeze(0),\n",
    "            image_np.shape[:2]\n",
    "        ).squeeze().numpy()\n",
    "\n",
    "        plt.imshow(image_np)\n",
    "        plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
    "        plt.title(f\"{name}\\nPred: {class_names[pred_class]}\\nConf: {confidence:.2%}\")\n",
    "        plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "-_-yPY-tul4a",
   "metadata": {
    "id": "-_-yPY-tul4a"
   },
   "source": [
    "misclassified examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "-_XEag8ek7hg",
   "metadata": {
    "collapsed": true,
    "id": "-_XEag8ek7hg",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=30, device=device)\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=44, device=device)\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=45, device=device)\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=50, device=device)\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=229, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5zXIx-vFuotS",
   "metadata": {
    "collapsed": true,
    "id": "5zXIx-vFuotS",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=400, device=device)\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=300, device=device)\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=500, device=device)\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=1000, device=device)\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=1100, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wkig69dGvlOY",
   "metadata": {
    "collapsed": true,
    "id": "wkig69dGvlOY",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=242, device=device)\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=361, device=device)\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=583, device=device)\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=1013, device=device)\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=20, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iYEyScqBwMxw",
   "metadata": {
    "id": "iYEyScqBwMxw"
   },
   "outputs": [],
   "source": [
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=400, device=device) #menings\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=415, device=device)\n",
    "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=425, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c0aca1",
   "metadata": {},
   "source": [
    "Grad-CAM activation map characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "iKE0w3Atk5Rk",
   "metadata": {
    "id": "iKE0w3Atk5Rk"
   },
   "outputs": [],
   "source": [
    "def analyze_gradcam_activation(heatmap, image):\n",
    "\n",
    "    # activation:\n",
    "    activation_mean = np.mean(heatmap)\n",
    "    activation_std = np.std(heatmap)\n",
    "\n",
    "    # coverage:\n",
    "    significant_threshold = 0.5  #threshold for significant activation\n",
    "    coverage = np.mean(heatmap > significant_threshold)\n",
    "\n",
    "    # focus (ratio of max activation to mean activation)\n",
    "    focus_ratio = np.max(heatmap) / (activation_mean + 1e-6)\n",
    "\n",
    "    return {\n",
    "        'mean_activation': float(activation_mean),\n",
    "        'activation_std': float(activation_std),\n",
    "        'coverage': float(coverage),\n",
    "        'focus_ratio': float(focus_ratio)\n",
    "    }\n",
    "\n",
    "#summ:\n",
    "def summarize_gradcam_results(models_dict, test_loader, class_names=None):\n",
    "    summary = {model_name: {'correct': 0, 'stats': []} for model_name in models_dict.keys()}\n",
    "    total_samples = len(test_loader.dataset)\n",
    "\n",
    "    print(f\"Analyzing {total_samples} images...\")\n",
    "\n",
    "    # Process all batches\n",
    "    for images, labels in tqdm(test_loader, desc=\"Processing batches\"):\n",
    "        for i in range(len(images)):\n",
    "            image = images[i]\n",
    "            true_label = labels[i].item()\n",
    "\n",
    "            for name, model in models_dict.items():\n",
    "                # Get prediction\n",
    "                with torch.no_grad():\n",
    "                    pred = model(image.unsqueeze(0))\n",
    "                    pred_class = pred.argmax().item()\n",
    "                    confidence = torch.softmax(pred, dim=1)[0, pred_class].item()\n",
    "\n",
    "                # Generate and analyze Grad-CAM\n",
    "                heatmap = generate_gradcam(model, image).detach().cpu().numpy()\n",
    "                heatmap = transforms.functional.resize(\n",
    "                    torch.from_numpy(heatmap).unsqueeze(0).unsqueeze(0),\n",
    "                    image.shape[1:]\n",
    "                ).squeeze().numpy()\n",
    "\n",
    "                # Analyze activation\n",
    "                stats = analyze_gradcam_activation(heatmap, image)\n",
    "                stats['confidence'] = confidence\n",
    "                stats['correct_prediction'] = (pred_class == true_label)\n",
    "\n",
    "                # Update summary\n",
    "                summary[name]['stats'].append(stats)\n",
    "                if pred_class == true_label:\n",
    "                    summary[name]['correct'] += 1\n",
    "\n",
    "    return format_gradcam_summary(summary, total_samples, class_names)\n",
    "\n",
    "def format_gradcam_summary(summary, total_samples, class_names):\n",
    "    \n",
    "    report = \"Grad-CAM Analysis Summary\\n\" + \"=\"*50 + \"\\n\\n\"\n",
    "    report += f\"Total samples analyzed: {total_samples}\\n\\n\"\n",
    "\n",
    "    for model_name, model_data in summary.items():\n",
    "        report += f\"\\nModel: {model_name}\\n{'-'*30}\\n\"\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = model_data['correct'] / total_samples\n",
    "        report += f\"Accuracy: {accuracy:.2%}\\n\"\n",
    "\n",
    "        # Average statistics\n",
    "        stats = model_data['stats']\n",
    "        avg_stats = {\n",
    "            'mean_activation': np.mean([s['mean_activation'] for s in stats]),\n",
    "            'activation_std': np.mean([s['activation_std'] for s in stats]),\n",
    "            'coverage': np.mean([s['coverage'] for s in stats]),\n",
    "            'focus_ratio': np.mean([s['focus_ratio'] for s in stats]),\n",
    "            'confidence': np.mean([s['confidence'] for s in stats])\n",
    "        }\n",
    "\n",
    "        report += f\"Average Activation: {avg_stats['mean_activation']:.3f}\\n\"\n",
    "        report += f\"Average Coverage: {avg_stats['coverage']:.2%}\\n\"\n",
    "        report += f\"Focus Ratio: {avg_stats['focus_ratio']:.2f}\\n\"\n",
    "        report += f\"Average Confidence: {avg_stats['confidence']:.2%}\\n\"\n",
    "\n",
    "        # Standard deviations\n",
    "        std_stats = {\n",
    "            'activation_std': np.std([s['mean_activation'] for s in stats]),\n",
    "            'coverage_std': np.std([s['coverage'] for s in stats]),\n",
    "            'focus_ratio_std': np.std([s['focus_ratio'] for s in stats]),\n",
    "            'confidence_std': np.std([s['confidence'] for s in stats])\n",
    "        }\n",
    "\n",
    "        report += f\"\\nVariability Analysis:\\n\"\n",
    "        report += f\"Activation Std: {std_stats['activation_std']:.3f}\\n\"\n",
    "        report += f\"Coverage Std: {std_stats['coverage_std']:.2%}\\n\"\n",
    "        report += f\"Focus Ratio Std: {std_stats['focus_ratio_std']:.2f}\\n\"\n",
    "        report += f\"Confidence Std: {std_stats['confidence_std']:.2%}\\n\"\n",
    "\n",
    "        # Analyze activation patterns\n",
    "        report += \"\\nActivation Analysis:\\n\"\n",
    "        if avg_stats['coverage'] > 0.7:\n",
    "            report += \"- Wide activation pattern (might be looking at too much)\\n\"\n",
    "        elif avg_stats['coverage'] < 0.3:\n",
    "            report += \"- Focused activation pattern (concentrated attention)\\n\"\n",
    "\n",
    "        if avg_stats['focus_ratio'] > 5:\n",
    "            report += \"- High focus ratio (very specific features)\\n\"\n",
    "        elif avg_stats['focus_ratio'] < 2:\n",
    "            report += \"- Low focus ratio (more distributed attention)\\n\"\n",
    "\n",
    "        # Add histogram visualization of key metrics\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        plt.subplot(2, 2, 1)\n",
    "        plt.hist([s['mean_activation'] for s in stats], bins=30)\n",
    "        plt.title('Distribution of Mean Activation')\n",
    "\n",
    "        plt.subplot(2, 2, 2)\n",
    "        plt.hist([s['coverage'] for s in stats], bins=30)\n",
    "        plt.title('Distribution of Coverage')\n",
    "\n",
    "        plt.subplot(2, 2, 3)\n",
    "        plt.hist([s['focus_ratio'] for s in stats], bins=30)\n",
    "        plt.title('Distribution of Focus Ratio')\n",
    "\n",
    "        plt.subplot(2, 2, 4)\n",
    "        plt.hist([s['confidence'] for s in stats], bins=30)\n",
    "        plt.title('Distribution of Confidence')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return report\n",
    "\n",
    "def run_gradcam_analysis(models_dict, test_loader, class_names):\n",
    "    summary = summarize_gradcam_results(\n",
    "        models_dict,\n",
    "        test_loader,\n",
    "        class_names=class_names\n",
    "    )\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sF50Wczlk7gl",
   "metadata": {
    "collapsed": true,
    "id": "sF50Wczlk7gl",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "run_gradcam_analysis(models_dict, test_loader_norm, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GXrc-ECdZwf9",
   "metadata": {
    "id": "GXrc-ECdZwf9"
   },
   "source": [
    "SSIM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "yh_sEheylU45",
   "metadata": {
    "id": "yh_sEheylU45"
   },
   "outputs": [],
   "source": [
    "def randomize_layer_weights(model, num_layers):\n",
    "    \"\"\"Randomize weights of last n layers\"\"\"\n",
    "    params = list(model.parameters())\n",
    "    for i in range(min(num_layers * 2, len(params))):  # *2 because each layer has weights and bias\n",
    "        original_weights = params[-(i+1)].data.clone()\n",
    "        params[-(i+1)].data = torch.randn_like(original_weights)\n",
    "    return original_weights, -(i+1)\n",
    "\n",
    "def reset_layer_weights(model, weights, layer_idx): #reset weights of a layer back to original\n",
    "    params = list(model.parameters())\n",
    "    params[layer_idx].data = weights\n",
    "\n",
    "def calculate_ssim_masks_batch(models_dict, test_loader, num_layers=6, num_samples=None): #SSIM\n",
    "\n",
    "    # Initialize storage for SSIM scores\n",
    "    ssim_scores = {name: [] for name in models_dict.keys()}\n",
    "    x_labels = ['original', 'classifier', 'denseblock4', 'denseblock3',\n",
    "                'denseblock2', 'denseblock1'][:num_layers+1]\n",
    "\n",
    "    # Count total samples to process\n",
    "    total_samples = len(test_loader.dataset) if num_samples is None else num_samples\n",
    "    samples_processed = 0\n",
    "\n",
    "    # Process each batch\n",
    "    for images, _ in tqdm(test_loader, desc=\"Processing images\"):\n",
    "        batch_scores = {name: [] for name in models_dict.keys()}\n",
    "\n",
    "        # Process each image in the batch\n",
    "        for image in images:\n",
    "            if samples_processed >= total_samples and num_samples is not None:\n",
    "                break\n",
    "\n",
    "            for name, model in models_dict.items():\n",
    "                # Generate original mask\n",
    "                original_mask = generate_gradcam(model, image).detach().cpu().numpy()\n",
    "                current_scores = [1.0]  # SSIM with itself = 1\n",
    "\n",
    "                # Store original weights for reset\n",
    "                original_weights = []\n",
    "                layer_indices = []\n",
    "\n",
    "                # Progressive randomization\n",
    "                for layer in range(num_layers):\n",
    "                    weights, idx = randomize_layer_weights(model, layer + 1)\n",
    "                    original_weights.append(weights)\n",
    "                    layer_indices.append(idx)\n",
    "\n",
    "                    # Generate new mask and calculate SSIM\n",
    "                    random_mask = generate_gradcam(model, image).detach().cpu().numpy()\n",
    "                    score = ssim(original_mask, random_mask, data_range=1.0)\n",
    "                    current_scores.append(score)\n",
    "\n",
    "                    # Reset weights for next iteration\n",
    "                    for w, i in zip(original_weights[:-1], layer_indices[:-1]):\n",
    "                        reset_layer_weights(model, w, i)\n",
    "\n",
    "                batch_scores[name].append(current_scores)\n",
    "\n",
    "            samples_processed += 1\n",
    "\n",
    "        # Aggregate batch results\n",
    "        for name in models_dict.keys():\n",
    "            if not ssim_scores[name]:  # First batch\n",
    "                ssim_scores[name] = [[] for _ in range(num_layers + 1)]\n",
    "            for scores in batch_scores[name]:\n",
    "                for i, score in enumerate(scores):\n",
    "                    ssim_scores[name][i].append(score)\n",
    "\n",
    "    # Calculate statistics\n",
    "    ssim_stats = {name: {\n",
    "        'mean': [np.mean(layer_scores) for layer_scores in model_scores],\n",
    "        'std': [np.std(layer_scores) for layer_scores in model_scores]\n",
    "    } for name, model_scores in ssim_scores.items()}\n",
    "\n",
    "    # Plot results with error bars\n",
    "    plt.figure(figsize=(12, 7))\n",
    "    for name, stats in ssim_stats.items():\n",
    "        plt.errorbar(range(len(stats['mean'])),\n",
    "                    stats['mean'],\n",
    "                    yerr=stats['std'],\n",
    "                    marker='o',\n",
    "                    label=f'{name}',\n",
    "                    capsize=5)\n",
    "\n",
    "    plt.xticks(range(len(x_labels)), x_labels, rotation=45)\n",
    "    plt.ylabel('SSIM score (mean ± std)')\n",
    "    plt.xlabel('Randomized layers')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.title(f'Sanity Check: SSIM Similarity vs. Layer Randomization\\n(n={samples_processed} images)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return ssim_stats\n",
    "\n",
    "\n",
    "def run_ssim_analysis(models_dict, test_loader, num_samples=None):\n",
    "    \n",
    "    stats = calculate_ssim_masks_batch(\n",
    "        models_dict,\n",
    "        test_loader,\n",
    "        num_layers=6,\n",
    "        num_samples=num_samples\n",
    "    )\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bZB3YDs2lX5O",
   "metadata": {
    "id": "bZB3YDs2lX5O"
   },
   "outputs": [],
   "source": [
    "stats = run_ssim_analysis(models_dict, test_loader_norm, num_samples=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hk2vumSeoJMo",
   "metadata": {
    "id": "hk2vumSeoJMo"
   },
   "source": [
    "# <a id='gen'>Sparsity using shap: </a>\n",
    "\n",
    "(clean the backward hook from grad-cam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ihAJnf2fat",
   "metadata": {
    "id": "51ihAJnf2fat"
   },
   "outputs": [],
   "source": [
    "def calculate_shap_sparsity(models_dict, test_loader_norm, class_names, num_samples=15):\n",
    "\n",
    "\n",
    "    sparsity_scores = {model_name: {class_name: [] for class_name in class_names}\n",
    "                      for model_name in models_dict.keys()}\n",
    "\n",
    "    # Collect samples per class\n",
    "    print(\"Collecting samples...\")\n",
    "    class_samples = {class_name: [] for class_name in class_names}\n",
    "    for images, labels in test_loader_norm:\n",
    "        for img, label in zip(images, labels):\n",
    "            class_name = class_names[label.item()]\n",
    "            if len(class_samples[class_name]) < num_samples:\n",
    "                class_samples[class_name].append(img)\n",
    "\n",
    "        # Check if enough samples:\n",
    "        if all(len(samples) >= num_samples for samples in class_samples.values()):\n",
    "            break\n",
    "\n",
    "    # Process each model\n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\nProcessing model: {model_name}\")\n",
    "        model.eval()\n",
    "\n",
    "        # Move model to GPU if available\n",
    "        if torch.cuda.is_available():\n",
    "            model = model.cuda()\n",
    "\n",
    "        # Process each class\n",
    "        for class_name in class_names:\n",
    "            print(f\"\\nProcessing class: {class_name}\")\n",
    "            samples = class_samples[class_name][:num_samples]\n",
    "\n",
    "            if not samples:\n",
    "                continue\n",
    "\n",
    "            # Process in small batches\n",
    "            batch_size = 5\n",
    "            for i in range(0, len(samples), batch_size):\n",
    "                batch = samples[i:i+batch_size]\n",
    "                batch = torch.stack(batch)\n",
    "\n",
    "                if torch.cuda.is_available():\n",
    "                    batch = batch.cuda()\n",
    "\n",
    "                try:\n",
    "                    # Create explainer for this batch\n",
    "                    background = torch.zeros_like(batch[0:1])\n",
    "                    explainer = shap.GradientExplainer(model, background)\n",
    "\n",
    "                    # Get SHAP values\n",
    "                    shap_values = explainer.shap_values(batch)\n",
    "\n",
    "                    # Process each image in batch\n",
    "                    for idx in range(len(batch)):\n",
    "                        if isinstance(shap_values, list):\n",
    "                            # For multi-class output, use predicted class\n",
    "                            with torch.no_grad():\n",
    "                                pred_class = model(batch[idx:idx+1]).argmax().item()\n",
    "                            shap_map = np.abs(shap_values[pred_class][idx])\n",
    "                        else:\n",
    "                            shap_map = np.abs(shap_values[idx])\n",
    "\n",
    "                        # Calculate sparsity with 5% threshold\n",
    "                        threshold = np.max(shap_map) * 0.05\n",
    "                        sparsity = np.mean(shap_map < threshold)\n",
    "                        sparsity_scores[model_name][class_name].append(sparsity)\n",
    "\n",
    "                        print(f\"Image {i+idx+1}/{len(samples)}: Sparsity = {sparsity:.3f}\")\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing batch: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "                finally:\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"\\nFinal Sparsity Scores:\")\n",
    "    for model_name in models_dict.keys():\n",
    "        print(f\"\\n{model_name}:\")\n",
    "        for class_name in class_names:\n",
    "            scores = sparsity_scores[model_name][class_name]\n",
    "            if scores:\n",
    "                avg_score = np.mean(scores)\n",
    "                std_score = np.std(scores)\n",
    "                print(f\"{class_name}: {avg_score:.3f} ± {std_score:.3f} (n={len(scores)})\")\n",
    "            else:\n",
    "                print(f\"{class_name}: No valid scores\")\n",
    "\n",
    "    return sparsity_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6PFiSZEV2hoC",
   "metadata": {
    "collapsed": true,
    "id": "6PFiSZEV2hoC",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "sparsity_scores = calculate_shap_sparsity(models_dict, test_loader_norm, class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "HWWeZlaXr2WK",
   "metadata": {
    "id": "HWWeZlaXr2WK"
   },
   "source": [
    "# <a id='gen'>Class-wise contrastivity - using shap: </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gu1on-B15Bx3",
   "metadata": {
    "id": "gu1on-B15Bx3"
   },
   "source": [
    "new version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Prx8e7jJ5AqM",
   "metadata": {
    "id": "Prx8e7jJ5AqM"
   },
   "outputs": [],
   "source": [
    "def calculate_class_contrast_matrix(models_dict, test_loader_norm, class_names, num_samples=10):\n",
    "    class_contrasts = {}\n",
    "\n",
    "    # Get all images and labels\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader_norm:\n",
    "            all_images.extend(images.cpu())\n",
    "            all_labels.extend(labels.cpu())\n",
    "    all_labels = np.array([label.item() for label in all_labels])\n",
    "\n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\nProcessing model: {model_name}\")\n",
    "        class_shap_values = {class_name: [] for class_name in class_names}\n",
    "\n",
    "        # Set model to eval mode\n",
    "        model.eval()\n",
    "\n",
    "        # Process each class\n",
    "        for class_idx, class_name in enumerate(class_names):\n",
    "            class_indices = np.where(all_labels == class_idx)[0]\n",
    "            selected_indices = np.random.choice(class_indices,\n",
    "                                             size=min(num_samples, len(class_indices)),\n",
    "                                             replace=False)\n",
    "\n",
    "            for img_idx in selected_indices:\n",
    "                image = all_images[img_idx].to(next(model.parameters()).device)\n",
    "\n",
    "                try:\n",
    "                    # Get SHAP values\n",
    "                    background = torch.zeros_like(image.unsqueeze(0))\n",
    "                    explainer = shap.GradientExplainer(model, background)\n",
    "                    shap_values = explainer.shap_values(image.unsqueeze(0))\n",
    "\n",
    "                    # Process SHAP values\n",
    "                    if isinstance(shap_values, list):\n",
    "                        shap_map = np.mean(np.abs(shap_values[0][0]), axis=-1)\n",
    "                    else:\n",
    "                        shap_map = np.mean(np.abs(shap_values[0]), axis=-1)\n",
    "\n",
    "                    class_shap_values[class_name].append(shap_map)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {img_idx}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        # Calculate average SHAP values per class\n",
    "        avg_shap_values = {\n",
    "            class_name: np.mean(values, axis=0) if values else None\n",
    "            for class_name, values in class_shap_values.items()\n",
    "        }\n",
    "\n",
    "        # Create contrastivity matrix\n",
    "        n_classes = len(class_names)\n",
    "        contrast_matrix = np.zeros((n_classes, n_classes))\n",
    "\n",
    "        for i, class1 in enumerate(class_names):\n",
    "            for j, class2 in enumerate(class_names):\n",
    "                if i != j and avg_shap_values[class1] is not None and avg_shap_values[class2] is not None:\n",
    "                    # Calculate contrast score (normalized absolute difference)\n",
    "                    diff = avg_shap_values[class1] - avg_shap_values[class2]\n",
    "                    #contrast_score = np.mean(np.abs(diff)) / (np.mean(np.abs(avg_shap_values[class1])) + np.mean(np.abs(avg_shap_values[class2])) + 1e-6)\n",
    "                    contrast_score = np.mean(np.abs(diff))  # Remove the normalization\n",
    "                    contrast_matrix[i, j] = contrast_score\n",
    "\n",
    "        # Print formatted table\n",
    "        print(f\"\\nClass-wise Contrastivity Scores for {model_name}\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"{'':15}\", end=\"\")\n",
    "        for class_name in class_names:\n",
    "            print(f\"{class_name:12}\", end=\"\")\n",
    "        print()\n",
    "\n",
    "        for i, class1 in enumerate(class_names):\n",
    "            print(f\"{class1:15}\", end=\"\")\n",
    "            for j, class2 in enumerate(class_names):\n",
    "                if i == j:\n",
    "                    print(f\"{'-':12}\", end=\"\")\n",
    "                else:\n",
    "                    print(f\"{contrast_matrix[i,j]:12.3f}\", end=\"\")\n",
    "            print()\n",
    "\n",
    "        class_contrasts[model_name] = contrast_matrix\n",
    "\n",
    "    return class_contrasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UoSQaFrl5YGF",
   "metadata": {
    "collapsed": true,
    "id": "UoSQaFrl5YGF",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    'base': base_model,\n",
    "    'trivial_aug': ta_model,\n",
    "    'gans': gan_model,\n",
    "    'combined': combined_model\n",
    "}\n",
    "\n",
    "\n",
    "class_names = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
    "\n",
    "contrast_matrices = calculate_class_contrast_matrix(\n",
    "    models_dict=models_dict,\n",
    "    test_loader_norm=test_loader_norm,\n",
    "    class_names=class_names,\n",
    "    num_samples=10  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iOq7KAkKIOYF",
   "metadata": {
    "id": "iOq7KAkKIOYF"
   },
   "source": [
    "contrastivity heatmaps using SHAP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QJBMkif-IN08",
   "metadata": {
    "id": "QJBMkif-IN08"
   },
   "outputs": [],
   "source": [
    "def create_heatmaps(models_dict, test_loader_norm, test_loader, class_names, num_samples=20):\n",
    "    \n",
    "    # Get all normalized images and labels\n",
    "    all_images_norm = []\n",
    "    all_labels = []\n",
    "\n",
    "    for images, labels in test_loader_norm:\n",
    "        all_images_norm.append(images)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "    all_images_norm = torch.cat(all_images_norm, dim=0)\n",
    "    all_labels = torch.cat(all_labels, dim=0).cpu().numpy()\n",
    "\n",
    "    fig = plt.figure(figsize=(24, 20))\n",
    "    gs = GridSpec(4, 4, figure=fig, hspace=0.3, wspace=0.3)\n",
    "    fig.suptitle('Enhanced Class-Specific SHAP Feature Importance Maps', fontsize=16, y=0.95) #more space now \n",
    "    all_shap_values = []\n",
    "\n",
    "    # First pass to collect all SHAP values\n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\nCollecting SHAP values for model: {model_name}\")\n",
    "        model.eval()\n",
    "\n",
    "        for class_idx, class_name in enumerate(class_names):\n",
    "            class_indices = np.where(all_labels == class_idx)[0]\n",
    "            selected_indices = np.random.choice(class_indices,\n",
    "                                             size=min(num_samples, len(class_indices)),\n",
    "                                             replace=False)\n",
    "\n",
    "            avg_shap_values = None\n",
    "            count = 0\n",
    "\n",
    "            for img_idx in selected_indices:\n",
    "                image = all_images_norm[img_idx].unsqueeze(0).to(next(model.parameters()).device)\n",
    "\n",
    "                try:\n",
    "                    background = torch.zeros_like(image)\n",
    "                    explainer = shap.GradientExplainer(model, background)\n",
    "                    shap_values = explainer.shap_values(image)\n",
    "\n",
    "                    if isinstance(shap_values, list):\n",
    "                        shap_map = np.mean(np.abs(shap_values[class_idx][0]), axis=0)\n",
    "                    else:\n",
    "                        shap_map = np.mean(np.abs(shap_values[0]), axis=0)\n",
    "\n",
    "                    if avg_shap_values is None:\n",
    "                        avg_shap_values = shap_map\n",
    "                    else:\n",
    "                        avg_shap_values += shap_map\n",
    "                    count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image {img_idx}: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if count > 0:\n",
    "                avg_shap_values /= count\n",
    "                all_shap_values.append(avg_shap_values)\n",
    "\n",
    "    # Get global min and max for consistent scaling\n",
    "    global_max = max(map(np.max, all_shap_values))\n",
    "\n",
    "    # Second pass to create plots with consistent scaling\n",
    "    shap_idx = 0\n",
    "    for model_idx, (model_name, model) in enumerate(models_dict.items()):\n",
    "        for class_idx, class_name in enumerate(class_names):\n",
    "            ax = fig.add_subplot(gs[model_idx, class_idx])\n",
    "\n",
    "            # Use imshow with enhanced settings\n",
    "            im = ax.imshow(all_shap_values[shap_idx],\n",
    "                         cmap='viridis',  # Changed to viridis for better visibility\n",
    "                         vmin=0,\n",
    "                         vmax=global_max)\n",
    "\n",
    "            ax.set_title(f'{model_name}\\n{class_name}', fontsize=12, pad=10)\n",
    "            ax.axis('off')\n",
    "\n",
    "            # Add colorbar with scientific notation\n",
    "            plt.colorbar(im, ax=ax, label='SHAP value', format='%.2e')\n",
    "            shap_idx += 1\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rJVMYBIKX5Me",
   "metadata": {
    "collapsed": true,
    "id": "rJVMYBIKX5Me",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "fig = create_heatmaps(models_dict, test_loader_norm, test_loader, class_names)\n",
    "plt.show() # 20 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "sYOUPqU1h7sY",
   "metadata": {
    "id": "sYOUPqU1h7sY"
   },
   "outputs": [],
   "source": [
    "def process_shap_values(shap_values, class_idx=None):\n",
    "    \n",
    "    # Print shape information for debugging\n",
    "    print(\"SHAP values type:\", type(shap_values))\n",
    "    if isinstance(shap_values, list):\n",
    "        print(\"SHAP values[0] shape:\", shap_values[0][0].shape)\n",
    "    else:\n",
    "        print(\"SHAP values shape:\", shap_values.shape)\n",
    "\n",
    "    if isinstance(shap_values, list):\n",
    "        if class_idx is not None:\n",
    "            # For classification, take the specified class\n",
    "            shap_map = shap_values[class_idx][0]\n",
    "        else:\n",
    "            # If no class specified, take the first class\n",
    "            shap_map = shap_values[0][0]\n",
    "    else:\n",
    "        shap_map = shap_values[0]\n",
    "\n",
    "    print(\"Shape before reduction:\", shap_map.shape)\n",
    "\n",
    "    # Handle different possible shapes \n",
    "    if len(shap_map.shape) == 4:  # (C, H, W, 3) or similar\n",
    "        # Average across channels and RGB if present\n",
    "        shap_map = np.mean(np.abs(shap_map), axis=(0, -1))\n",
    "    elif len(shap_map.shape) == 3:  # (C, H, W) or (H, W, 3)\n",
    "        if shap_map.shape[-1] == 3:  # RGB\n",
    "            shap_map = np.mean(np.abs(shap_map), axis=-1)\n",
    "        else:  # Channels first\n",
    "            shap_map = np.mean(np.abs(shap_map), axis=0)\n",
    "\n",
    "    print(\"Shape after reduction:\", shap_map.shape)\n",
    "    return shap_map\n",
    "\n",
    "def create_enhanced_contrast_heatmaps(models_dict, test_loader_norm, class_names, num_samples=20): #Create enhanced SHAP feature importance heatmaps using class contrasts\n",
    "\n",
    "    # Collect samples per class\n",
    "    class_samples = {class_name: {'images': [], 'count': 0} for class_name in class_names}\n",
    "\n",
    "    print(\"Collecting samples...\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader_norm:\n",
    "            for img, label in zip(images, labels):\n",
    "                class_name = class_names[label.item()]\n",
    "                if class_samples[class_name]['count'] < num_samples:\n",
    "                    class_samples[class_name]['images'].append(img)\n",
    "                    class_samples[class_name]['count'] += 1\n",
    "\n",
    "            if all(samples['count'] >= num_samples for samples in class_samples.values()):\n",
    "                break\n",
    "\n",
    "    # Calculate number of comparisons\n",
    "    num_comparisons = len(list(itertools.combinations(class_names, 2)))\n",
    "    num_models = len(models_dict)\n",
    "\n",
    "    # Create figure\n",
    "    fig = plt.figure(figsize=(20, 5 * num_models))\n",
    "    gs = GridSpec(num_models, num_comparisons, figure=fig, hspace=0.4, wspace=0.3)\n",
    "    fig.suptitle('Class Contrast SHAP Feature Importance Maps', fontsize=16, y=0.95)\n",
    "\n",
    "    for model_idx, (model_name, model) in enumerate(models_dict.items()):\n",
    "        print(f\"\\nProcessing model: {model_name}\")\n",
    "        model.eval()\n",
    "\n",
    "        # Calculate average SHAP values for each class\n",
    "        class_avg_shap = {}\n",
    "\n",
    "        for class_name in class_names:\n",
    "            print(f\"Processing class: {class_name}\")\n",
    "            if not class_samples[class_name]['images']:\n",
    "                print(f\"Warning: No samples for class {class_name}\")\n",
    "                continue\n",
    "\n",
    "            avg_shap_values = None\n",
    "            count = 0\n",
    "\n",
    "            for image in class_samples[class_name]['images']:\n",
    "                try:\n",
    "                    device = next(model.parameters()).device\n",
    "                    image = image.to(device).unsqueeze(0)\n",
    "\n",
    "                    background = torch.zeros_like(image)\n",
    "                    explainer = shap.GradientExplainer(model, background)\n",
    "                    shap_values = explainer.shap_values(image)\n",
    "\n",
    "                    # Process SHAP values\n",
    "                    shap_map = process_shap_values(shap_values)\n",
    "\n",
    "                    if avg_shap_values is None:\n",
    "                        avg_shap_values = shap_map\n",
    "                    else:\n",
    "                        avg_shap_values += shap_map\n",
    "                    count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if count > 0:\n",
    "                avg_shap_values /= count\n",
    "                class_avg_shap[class_name] = avg_shap_values\n",
    "\n",
    "        # Create contrast visualizations\n",
    "        for comp_idx, (class1, class2) in enumerate(itertools.combinations(class_names, 2)):\n",
    "            if class1 not in class_avg_shap or class2 not in class_avg_shap:\n",
    "                print(f\"Skipping {class1} vs {class2} - missing values\")\n",
    "                continue\n",
    "\n",
    "            # Calculate contrast\n",
    "            contrast = class_avg_shap[class1] - class_avg_shap[class2]\n",
    "\n",
    "            # Normalize contrast to [-1, 1]\n",
    "            vmax = np.max(np.abs(contrast))\n",
    "            if vmax < 1e-6:\n",
    "                print(f\"Warning: Very small contrast between {class1} and {class2}\")\n",
    "                continue\n",
    "\n",
    "            contrast_normalized = contrast / (vmax + 1e-7)\n",
    "\n",
    "            # Create subplot\n",
    "            ax = fig.add_subplot(gs[model_idx, comp_idx])\n",
    "            # Ensure contrast is 2D and properly normalized\n",
    "            if len(contrast_normalized.shape) > 2:\n",
    "                print(f\"Warning: Unexpected shape {contrast_normalized.shape}, reducing dimensions\")\n",
    "                contrast_normalized = np.mean(contrast_normalized, axis=tuple(range(len(contrast_normalized.shape)-2)))\n",
    "\n",
    "            im = ax.imshow(contrast_normalized,\n",
    "                          cmap='RdBu_r',\n",
    "                          vmin=-1,\n",
    "                          vmax=1,\n",
    "                          interpolation='nearest')\n",
    "\n",
    "            ax.set_title(f'{model_name}\\n{class1} vs {class2}', fontsize=10)\n",
    "            plt.colorbar(im, ax=ax, format='%.2e')\n",
    "            ax.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    return fig\n",
    "\n",
    "def visualize_single_class_features(models_dict, test_loader_norm, class_names, num_samples=20): #individual class feature importance heatmaps\n",
    "\n",
    "    num_models = len(models_dict)\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    fig = plt.figure(figsize=(4 * num_classes, 4 * num_models))\n",
    "    gs = GridSpec(num_models, num_classes, figure=fig, hspace=0.3, wspace=0.3)\n",
    "    fig.suptitle('Single Class SHAP Feature Importance Maps', fontsize=16, y=0.95)\n",
    "\n",
    "    # Collect samples per class\n",
    "    class_samples = {class_name: {'images': [], 'count': 0} for class_name in class_names}\n",
    "\n",
    "    print(\"Collecting samples...\")\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader_norm:\n",
    "            for img, label in zip(images, labels):\n",
    "                class_name = class_names[label.item()]\n",
    "                if class_samples[class_name]['count'] < num_samples:\n",
    "                    class_samples[class_name]['images'].append(img)\n",
    "                    class_samples[class_name]['count'] += 1\n",
    "\n",
    "            if all(samples['count'] >= num_samples for samples in class_samples.values()):\n",
    "                break\n",
    "\n",
    "    all_shap_values = []\n",
    "\n",
    "    for model_idx, (model_name, model) in enumerate(models_dict.items()):\n",
    "        print(f\"\\nProcessing model: {model_name}\")\n",
    "        model.eval()\n",
    "\n",
    "        for class_idx, class_name in enumerate(class_names):\n",
    "            if not class_samples[class_name]['images']:\n",
    "                print(f\"Warning: No samples for class {class_name}\")\n",
    "                continue\n",
    "\n",
    "            avg_shap_values = None\n",
    "            count = 0\n",
    "\n",
    "            for image in class_samples[class_name]['images']:\n",
    "                try:\n",
    "                    device = next(model.parameters()).device\n",
    "                    image = image.to(device).unsqueeze(0)\n",
    "\n",
    "                    background = torch.zeros_like(image)\n",
    "                    explainer = shap.GradientExplainer(model, background)\n",
    "                    shap_values = explainer.shap_values(image)\n",
    "\n",
    "                    # Process SHAP values\n",
    "                    shap_map = process_shap_values(shap_values, class_idx)\n",
    "\n",
    "                    if avg_shap_values is None:\n",
    "                        avg_shap_values = shap_map\n",
    "                    else:\n",
    "                        avg_shap_values += shap_map\n",
    "                    count += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing image: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "            if count > 0:\n",
    "                avg_shap_values /= count\n",
    "                all_shap_values.append(avg_shap_values)\n",
    "\n",
    "                # Normalize values to [0, 1]\n",
    "                normalized_values = avg_shap_values / (np.max(avg_shap_values) + 1e-7)\n",
    "\n",
    "                ax = fig.add_subplot(gs[model_idx, class_idx])\n",
    "                # Ensure values are 2D\n",
    "                if len(normalized_values.shape) > 2:\n",
    "                    print(f\"Warning: Unexpected shape {normalized_values.shape}, reducing dimensions\")\n",
    "                    normalized_values = np.mean(normalized_values, axis=tuple(range(len(normalized_values.shape)-2)))\n",
    "\n",
    "                im = ax.imshow(normalized_values,\n",
    "                             cmap='viridis',\n",
    "                             vmin=0,\n",
    "                             vmax=1,\n",
    "                             interpolation='nearest')\n",
    "\n",
    "                ax.set_title(f'{model_name}\\n{class_name}', fontsize=10)\n",
    "                plt.colorbar(im, ax=ax, format='%.2e')\n",
    "                ax.axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t6uyeGLRk4VN",
   "metadata": {
    "collapsed": true,
    "id": "t6uyeGLRk4VN",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "contrast_fig = create_enhanced_contrast_heatmaps(\n",
    "    models_dict,\n",
    "    test_loader_norm,\n",
    "    class_names,\n",
    "    num_samples=5  #fewer samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q0ZOgAwHlCmp",
   "metadata": {
    "collapsed": true,
    "id": "q0ZOgAwHlCmp",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "contrast_fig = create_enhanced_contrast_heatmaps(\n",
    "    models_dict,\n",
    "    test_loader_norm,\n",
    "    class_names,\n",
    "    num_samples=20\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9359842f",
   "metadata": {},
   "source": [
    "<a id='gen'>quantitative analysis of SHAP value characteristics across different models and classes </a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8qTwIa9T5VHS",
   "metadata": {
    "id": "8qTwIa9T5VHS"
   },
   "source": [
    "Sparsity metrics (Gini index, top 10% contribution)\n",
    "Class contrast metrics (KL divergence, Wasserstein distance)\n",
    "Distribution characteristics (mean, std, percentiles)\n",
    "\n",
    "+\n",
    "\n",
    "Bar plots of sparsity metrics\n",
    "KL divergence between classes\n",
    "Box plots of SHAP value distributions\n",
    "\n",
    "\n",
    "Focus on view-independent metrics!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hJuArZHk5aQO",
   "metadata": {
    "collapsed": true,
    "id": "hJuArZHk5aQO",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def analyze_shap_distributions(models_dict, test_loader_norm, class_names, num_samples=20):\n",
    "  \n",
    "    def calculate_sparsity_metrics(shap_values):\n",
    "    \n",
    "        # Flatten SHAP values\n",
    "        flat_shap = np.abs(shap_values).flatten()\n",
    "\n",
    "        # Calculate metrics\n",
    "        gini = 1 - np.sum((flat_shap / np.sum(flat_shap)) ** 2)\n",
    "        top_10_percent = np.sum(np.sort(flat_shap)[-int(len(flat_shap)*0.1):]) / np.sum(flat_shap)\n",
    "        sparsity = np.mean(flat_shap < np.max(flat_shap) * 0.1)  # Fraction of small values\n",
    "\n",
    "        return {\n",
    "            'gini': gini,\n",
    "            'top_10_percent': top_10_percent,\n",
    "            'sparsity': sparsity\n",
    "        }\n",
    "\n",
    "    def calculate_class_contrast_metrics(shap_values_dict):\n",
    "        \n",
    "        contrasts = {}\n",
    "        for c1 in class_names:\n",
    "            for c2 in class_names:\n",
    "                if c1 < c2:\n",
    "                    # Calculate KL divergence between SHAP distributions\n",
    "                    s1 = np.abs(shap_values_dict[c1]).flatten()\n",
    "                    s2 = np.abs(shap_values_dict[c2]).flatten()\n",
    "\n",
    "                    # Normalize to probability distributions\n",
    "                    s1 = s1 / np.sum(s1)\n",
    "                    s2 = s2 / np.sum(s2)\n",
    "\n",
    "                    # Add small epsilon to avoid division by zero\n",
    "                    epsilon = 1e-10\n",
    "                    s1 += epsilon\n",
    "                    s2 += epsilon\n",
    "                    s1 /= np.sum(s1)\n",
    "                    s2 /= np.sum(s2)\n",
    "\n",
    "                    # Calculate symmetric KL divergence\n",
    "                    kl_div = (entropy(s1, s2) + entropy(s2, s1)) / 2\n",
    "\n",
    "                    # Calculate Wasserstein distance (approximation using sorted values)\n",
    "                    s1_sorted = np.sort(s1)\n",
    "                    s2_sorted = np.sort(s2)\n",
    "                    wasserstein = np.mean(np.abs(s1_sorted - s2_sorted))\n",
    "\n",
    "                    contrasts[f\"{c1}_vs_{c2}\"] = {\n",
    "                        'kl_divergence': kl_div,\n",
    "                        'wasserstein': wasserstein\n",
    "                    }\n",
    "        return contrasts\n",
    "\n",
    "    # Store results for each model\n",
    "    results = {}\n",
    "\n",
    "    # Process each model\n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\nProcessing model: {model_name}\")\n",
    "        model.eval()\n",
    "\n",
    "        # Collect SHAP values per class\n",
    "        class_shap_values = {class_name: [] for class_name in class_names}\n",
    "\n",
    "        # Process samples\n",
    "        sample_count = {class_name: 0 for class_name in class_names}\n",
    "\n",
    "        for images, labels in test_loader_norm:\n",
    "            if all(count >= num_samples for count in sample_count.values()):\n",
    "                break\n",
    "\n",
    "            for img, label in zip(images, labels):\n",
    "                class_name = class_names[label.item()]\n",
    "                if sample_count[class_name] >= num_samples:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Get SHAP values\n",
    "                    background = torch.zeros_like(img.unsqueeze(0))\n",
    "                    explainer = shap.GradientExplainer(model, background)\n",
    "                    shap_values = explainer.shap_values(img.unsqueeze(0))\n",
    "\n",
    "                    # Process SHAP values\n",
    "                    if isinstance(shap_values, list):\n",
    "                        processed_shap = np.mean(np.abs(shap_values[0][0]), axis=0)\n",
    "                    else:\n",
    "                        processed_shap = np.mean(np.abs(shap_values[0]), axis=0)\n",
    "\n",
    "                    class_shap_values[class_name].append(processed_shap)\n",
    "                    sample_count[class_name] += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing sample: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        # Calculate metrics\n",
    "        model_results = {\n",
    "            'sparsity': {},\n",
    "            'contrasts': {},\n",
    "            'distributions': {}\n",
    "        }\n",
    "\n",
    "        # Average SHAP values per class\n",
    "        avg_shap_values = {}\n",
    "        for class_name, values in class_shap_values.items():\n",
    "            if values:\n",
    "                avg_shap_values[class_name] = np.mean(values, axis=0)\n",
    "                # Calculate sparsity metrics\n",
    "                model_results['sparsity'][class_name] = calculate_sparsity_metrics(avg_shap_values[class_name])\n",
    "\n",
    "        # Calculate contrast metrics\n",
    "        model_results['contrasts'] = calculate_class_contrast_metrics(avg_shap_values)\n",
    "\n",
    "        # Store distribution information\n",
    "        for class_name, values in class_shap_values.items():\n",
    "            if values:\n",
    "                flat_values = np.concatenate([v.flatten() for v in values])\n",
    "                model_results['distributions'][class_name] = {\n",
    "                    'mean': np.mean(flat_values),\n",
    "                    'std': np.std(flat_values),\n",
    "                    'percentiles': np.percentile(flat_values, [25, 50, 75])\n",
    "                }\n",
    "\n",
    "        results[model_name] = model_results\n",
    "\n",
    "    return results\n",
    "\n",
    "def visualize_metrics(results):\n",
    "    \n",
    "    num_models = len(results)\n",
    "    fig = plt.figure(figsize=(15, 5 * num_models))\n",
    "    gs = GridSpec(num_models, 3, figure=fig)\n",
    "\n",
    "    for i, (model_name, model_results) in enumerate(results.items()):\n",
    "        # Plot sparsity metrics\n",
    "        ax1 = fig.add_subplot(gs[i, 0])\n",
    "        sparsity_data = []\n",
    "        labels = []\n",
    "        metrics = []\n",
    "        for class_name, metrics_dict in model_results['sparsity'].items():\n",
    "            for metric_name, value in metrics_dict.items():\n",
    "                sparsity_data.append(value)\n",
    "                labels.append(class_name)\n",
    "                metrics.append(metric_name)\n",
    "\n",
    "        ax1.bar(range(len(sparsity_data)), sparsity_data)\n",
    "        ax1.set_xticks(range(len(sparsity_data)))\n",
    "        ax1.set_xticklabels([f\"{l}\\n{m}\" for l, m in zip(labels, metrics)], rotation=45)\n",
    "        ax1.set_title(f\"{model_name} - Sparsity Metrics\")\n",
    "\n",
    "        # Plot contrast metrics\n",
    "        ax2 = fig.add_subplot(gs[i, 1])\n",
    "        contrast_data = []\n",
    "        contrast_labels = []\n",
    "        for pair, metrics in model_results['contrasts'].items():\n",
    "            contrast_data.append(metrics['kl_divergence'])\n",
    "            contrast_labels.append(pair)\n",
    "\n",
    "        ax2.bar(range(len(contrast_data)), contrast_data)\n",
    "        ax2.set_xticks(range(len(contrast_data)))\n",
    "        ax2.set_xticklabels(contrast_labels, rotation=45)\n",
    "        ax2.set_title(f\"{model_name} - KL Divergence Between Classes\")\n",
    "\n",
    "        # Plot distribution metrics\n",
    "        ax3 = fig.add_subplot(gs[i, 2])\n",
    "        for class_name, dist_metrics in model_results['distributions'].items():\n",
    "            ax3.boxplot([dist_metrics['percentiles']], positions=[list(model_results['distributions'].keys()).index(class_name)],\n",
    "                       labels=[class_name])\n",
    "        ax3.set_title(f\"{model_name} - SHAP Value Distributions\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IiPmyqpMGmsT",
   "metadata": {
    "id": "IiPmyqpMGmsT"
   },
   "outputs": [],
   "source": [
    "models_dict = {\n",
    "    'base': base_model,\n",
    "    'trivial_aug': ta_model,\n",
    "    'gans': gan_model,\n",
    "    'combined': combined_model\n",
    "}\n",
    "\n",
    "\n",
    "class_names = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
    "\n",
    "results = analyze_shap_distributions(\n",
    "    models_dict,\n",
    "    test_loader_norm,\n",
    "    class_names,\n",
    "    num_samples=20\n",
    ")\n",
    "\n",
    "fig = visualize_metrics(results)\n",
    "plt.savefig('shap_analysis.png', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rJOPO4itd52K",
   "metadata": {
    "id": "rJOPO4itd52K"
   },
   "outputs": [],
   "source": [
    "def create_shap_summary_plots(models_dict, test_loader_norm, class_names, num_samples=20):\n",
    "    \n",
    "    def process_shap_values(shap_values):\n",
    "        if isinstance(shap_values, list):\n",
    "            return shap_values[0][0]  #get first class\n",
    "        return shap_values[0]\n",
    "\n",
    "    def calculate_feature_stats(shap_map):\n",
    "\n",
    "        abs_shap = np.abs(shap_map)\n",
    "        return {\n",
    "            'mean_importance': np.mean(abs_shap),\n",
    "            'max_importance': np.max(abs_shap),\n",
    "            'percentile_95': np.percentile(abs_shap, 95),\n",
    "            'sparsity': np.mean(abs_shap < np.max(abs_shap) * 0.1)\n",
    "        }\n",
    "\n",
    "    results = {}\n",
    "    for model_name, model in models_dict.items():\n",
    "        print(f\"\\nProcessing model: {model_name}\")\n",
    "        model.eval()\n",
    "\n",
    "        # Collect samples and SHAP values per class\n",
    "        class_stats = {class_name: [] for class_name in class_names}\n",
    "        shap_magnitudes = {class_name: [] for class_name in class_names}\n",
    "\n",
    "        sample_count = {class_name: 0 for class_name in class_names}\n",
    "\n",
    "        for images, labels in test_loader_norm:\n",
    "            if all(count >= num_samples for count in sample_count.values()):\n",
    "                break\n",
    "\n",
    "            for img, label in zip(images, labels):\n",
    "                class_name = class_names[label.item()]\n",
    "                if sample_count[class_name] >= num_samples:\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    # Calculate SHAP values\n",
    "                    background = torch.zeros_like(img.unsqueeze(0))\n",
    "                    explainer = shap.GradientExplainer(model, background)\n",
    "                    shap_values = explainer.shap_values(img.unsqueeze(0))\n",
    "\n",
    "                    # Process SHAP values\n",
    "                    shap_map = process_shap_values(shap_values)\n",
    "                    stats = calculate_feature_stats(shap_map)\n",
    "                    class_stats[class_name].append(stats)\n",
    "\n",
    "                    # Store magnitudes for distribution plots\n",
    "                    shap_magnitudes[class_name].extend(np.abs(shap_map).flatten())\n",
    "\n",
    "                    sample_count[class_name] += 1\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing sample: {str(e)}\")\n",
    "                    continue\n",
    "\n",
    "        results[model_name] = {\n",
    "            'stats': class_stats,\n",
    "            'magnitudes': shap_magnitudes\n",
    "        }\n",
    "\n",
    "    \n",
    "    num_models = len(results)\n",
    "    fig = plt.figure(figsize=(15, 5 * num_models))\n",
    "    gs = GridSpec(num_models, 3, figure=fig)\n",
    "\n",
    "    for model_idx, (model_name, model_results) in enumerate(results.items()):\n",
    "        # 1. Feature Importance Violin Plot\n",
    "        ax1 = fig.add_subplot(gs[model_idx, 0])\n",
    "        violin_data = []\n",
    "        violin_positions = []\n",
    "        violin_labels = []\n",
    "\n",
    "        for i, (class_name, magnitudes) in enumerate(model_results['magnitudes'].items()):\n",
    "            violin_data.append(magnitudes)\n",
    "            violin_positions.append(i)\n",
    "            violin_labels.append(class_name)\n",
    "\n",
    "        ax1.violinplot(violin_data, positions=violin_positions)\n",
    "        ax1.set_xticks(violin_positions)\n",
    "        ax1.set_xticklabels(violin_labels, rotation=45)\n",
    "        ax1.set_title(f\"{model_name} - Feature Importance Distributions\")\n",
    "\n",
    "        # 2. Summary Statistics\n",
    "        ax2 = fig.add_subplot(gs[model_idx, 1])\n",
    "        for stat_name in ['mean_importance', 'max_importance', 'percentile_95']:\n",
    "            stat_values = []\n",
    "            for class_name in class_names:\n",
    "                class_stat = np.mean([stats[stat_name]\n",
    "                                    for stats in model_results['stats'][class_name]])\n",
    "                stat_values.append(class_stat)\n",
    "\n",
    "            ax2.plot(class_names, stat_values,\n",
    "                    marker='o',\n",
    "                    label=stat_name.replace('_', ' ').title())\n",
    "\n",
    "        ax2.set_xticklabels(class_names, rotation=45)\n",
    "        ax2.legend()\n",
    "        ax2.set_title(f\"{model_name} - Summary Statistics\")\n",
    "\n",
    "        # 3. Sparsity Comparison\n",
    "        ax3 = fig.add_subplot(gs[model_idx, 2])\n",
    "        sparsity_values = []\n",
    "        for class_name in class_names:\n",
    "            sparsity = np.mean([stats['sparsity']\n",
    "                              for stats in model_results['stats'][class_name]])\n",
    "            sparsity_values.append(sparsity)\n",
    "\n",
    "        ax3.bar(class_names, sparsity_values)\n",
    "        ax3.set_xticklabels(class_names, rotation=45)\n",
    "        ax3.set_title(f\"{model_name} - Feature Sparsity\")\n",
    "        ax3.set_ylim(0, 1)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ggvPZuepd-ET",
   "metadata": {
    "collapsed": true,
    "id": "ggvPZuepd-ET",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "summary_fig = create_shap_summary_plots(\n",
    "    models_dict,\n",
    "    test_loader_norm,\n",
    "    class_names,\n",
    "    num_samples=20\n",
    ")\n",
    "summary_fig.savefig('shap_summary.png', bbox_inches='tight', dpi=300)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "3yihh0wgGRcp"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
