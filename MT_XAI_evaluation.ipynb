{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "635f63ea",
      "metadata": {
        "collapsed": true,
        "id": "635f63ea"
      },
      "outputs": [],
      "source": [
        "!pip install lime\n",
        "!pip install git+https://github.com/jacobgil/pytorch-grad-cam.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e1ef1b",
      "metadata": {
        "id": "74e1ef1b"
      },
      "outputs": [],
      "source": [
        "# libraries:\n",
        "import copy\n",
        "import itertools\n",
        "import os\n",
        "import random\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import entropy\n",
        "\n",
        "# Deep Learning - PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "\n",
        "# Computer Vision & Image Processing\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from skimage import segmentation\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.segmentation import felzenszwalb\n",
        "from skimage.transform import resize\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.colors import TwoSlopeNorm\n",
        "from matplotlib.gridspec import GridSpec\n",
        "import seaborn as sns\n",
        "\n",
        "# Machine Learning Metrics\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Explainability Frameworks\n",
        "import lime\n",
        "from lime import lime_image\n",
        "from lime.wrappers.scikit_image import SegmentationAlgorithm\n",
        "import shap\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "# Suppress Warnings\n",
        "warnings.filterwarnings('ignore', message='unrecognized nn.Module: Flatten')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGgebZgx6v9W",
        "outputId": "2b037f43-3fcd-48dd-ec10-d70c13997417"
      },
      "id": "tGgebZgx6v9W",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA + preprocessing:"
      ],
      "metadata": {
        "id": "K-QxaR9d6yRT"
      },
      "id": "K-QxaR9d6yRT"
    },
    {
      "cell_type": "code",
      "source": [
        "train_dir = 'drive/MyDrive/MT/archive/Training'\n",
        "test_dir = 'drive/MyDrive/MT/archive/Testing'"
      ],
      "metadata": {
        "id": "YLG7WpTX6x5N"
      },
      "id": "YLG7WpTX6x5N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define transformations\n",
        "train_transform = transforms.Compose([\n",
        "    #transforms.TrivialAugmentWide(),\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.1848, 0.1848, 0.1848],\n",
        "                           std=[0.1768, 0.1768, 0.1768])\n",
        "])\n",
        "\n",
        "val_test_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.1848, 0.1848, 0.1848],\n",
        "                           std=[0.1768, 0.1768, 0.1768])\n",
        "])\n",
        "\n",
        "#for visualization:\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((128, 128)),\n",
        "    transforms.ToTensor(),\n",
        "])"
      ],
      "metadata": {
        "id": "2S9zD0DU66Ks"
      },
      "id": "2S9zD0DU66Ks",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training dataset with augmentation\n",
        "train_dataset = datasets.ImageFolder(train_dir, transform=train_transform)\n",
        "\n",
        "# Validation dataset without augmentation\n",
        "full_dataset = datasets.ImageFolder(train_dir, transform=val_test_transform)\n",
        "\n",
        "# Testing dataset\n",
        "test_dataset_norm = datasets.ImageFolder(test_dir, transform=val_test_transform)\n",
        "test_dataset = datasets.ImageFolder(test_dir, transform=test_transform)"
      ],
      "metadata": {
        "id": "9PZPD21X68KZ"
      },
      "id": "9PZPD21X68KZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split train/validation\n",
        "train_size = int(0.8 * len(full_dataset))\n",
        "val_size = len(full_dataset) - train_size\n",
        "\n",
        "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
        "\n",
        "# Loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "test_loader_norm = DataLoader(test_dataset_norm, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "Hk9Pv3HS8U4V"
      },
      "id": "Hk9Pv3HS8U4V",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class-weights:"
      ],
      "metadata": {
        "id": "yUKVKOGu7S0t"
      },
      "id": "yUKVKOGu7S0t"
    },
    {
      "cell_type": "code",
      "source": [
        "#Class weights: {0: 4.323996971990916, 1: 4.265870052277819, 2: 3.581191222570533, 3: 3.9203843514070007}\n",
        "\n",
        "# Calculate class weights\n",
        "train_counts = {\n",
        "    0: 1321,  # Glioma\n",
        "    1: 1339,  # Meningioma\n",
        "    2: 1595,  # No Tumor\n",
        "    3: 1457   # Pituitary\n",
        "}\n",
        "\n",
        "total_samples = sum(train_counts.values())  # 5712\n",
        "class_weights = {class_idx: total_samples / count\n",
        "                for class_idx, count in train_counts.items()}\n",
        "\n",
        "print(\"Class weights:\", class_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vtkQAsgC7Qzg",
        "outputId": "9cecc14d-b75e-404d-f5e8-d038ed6605fe"
      },
      "id": "vtkQAsgC7Qzg",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class weights: {0: 4.323996971990916, 1: 4.265870052277819, 2: 3.581191222570533, 3: 3.9203843514070007}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODEL:"
      ],
      "metadata": {
        "id": "iEQQQbG87cV5"
      },
      "id": "iEQQQbG87cV5"
    },
    {
      "cell_type": "code",
      "source": [
        "class BrainCNN(nn.Module):\n",
        "    def __init__(self, num_classes=4):\n",
        "        super(BrainCNN, self).__init__()\n",
        "\n",
        "        self.features = nn.Sequential(\n",
        "            nn.Conv2d(3, 16, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(16),\n",
        "            nn.Dropout2d(0.2),\n",
        "\n",
        "            nn.Conv2d(16, 32, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Dropout2d(0.2),\n",
        "\n",
        "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.Dropout2d(0.2)\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(64 * 16 * 16, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "def train_and_validate(model, train_loader, val_loader, test_loader, class_weights=None, epochs=10):\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    # Calculate class weights if not provided\n",
        "    if class_weights is None:\n",
        "        class_weights = class_weights\n",
        "\n",
        "    # Convert class weights to tensor\n",
        "    weight_tensor = torch.FloatTensor([class_weights[i] for i in range(len(class_weights))]).to(device)\n",
        "\n",
        "    # Initialize weighted loss function\n",
        "    criterion = nn.CrossEntropyLoss(weight=weight_tensor)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
        "\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'train_acc': [],\n",
        "        'val_loss': [],\n",
        "        'val_acc': []\n",
        "    }\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        train_correct = 0\n",
        "        train_total = 0\n",
        "\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            train_total += labels.size(0)\n",
        "            train_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_train_loss = train_loss/len(train_loader)\n",
        "        epoch_train_acc = 100 * train_correct / train_total\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        val_correct = 0\n",
        "        val_total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_loss += loss.item()\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                val_total += labels.size(0)\n",
        "                val_correct += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_val_loss = val_loss/len(val_loader)\n",
        "        epoch_val_acc = 100 * val_correct / val_total\n",
        "\n",
        "        history['train_loss'].append(epoch_train_loss)\n",
        "        history['train_acc'].append(epoch_train_acc)\n",
        "        history['val_loss'].append(epoch_val_loss)\n",
        "        history['val_acc'].append(epoch_val_acc)\n",
        "\n",
        "        print(f'Epoch {epoch+1}:')\n",
        "        print(f'Train Loss: {epoch_train_loss:.4f}, Train Accuracy: {epoch_train_acc:.2f}%')\n",
        "        print(f'Val Loss: {epoch_val_loss:.4f}, Val Accuracy: {epoch_val_acc:.2f}%')\n",
        "\n",
        "    # Final Test Phase\n",
        "    model.eval()\n",
        "    test_correct = 0\n",
        "    test_total = 0\n",
        "    test_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            test_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            test_total += labels.size(0)\n",
        "            test_correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('\\nFinal Test Results:')\n",
        "    print(f'Test Loss: {test_loss/len(test_loader):.4f}')\n",
        "    print(f'Test Accuracy: {100 * test_correct / test_total:.2f}%')\n",
        "\n",
        "    return history, model"
      ],
      "metadata": {
        "id": "tE5JYu8K7am3"
      },
      "id": "tE5JYu8K7am3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "load models:"
      ],
      "metadata": {
        "id": "9PhOxSiU7fOS"
      },
      "id": "9PhOxSiU7fOS"
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model_from_checkpoint(checkpoint_path, model_class):\n",
        "    \"\"\"\n",
        "    Load a model from a checkpoint file.\n",
        "\n",
        "    Args:\n",
        "        checkpoint_path (str): Path to the checkpoint file\n",
        "        model_class (nn.Module): The model class to instantiate\n",
        "\n",
        "    Returns:\n",
        "        tuple: (loaded_model, history)\n",
        "    \"\"\"\n",
        "    # Load the full checkpoint\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n",
        "\n",
        "    # Initialize model\n",
        "    model = model_class()\n",
        "\n",
        "    # Load the model state dict\n",
        "    if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        history = checkpoint.get('history', None)\n",
        "    else:\n",
        "        # If the checkpoint is just the state dict\n",
        "        model.load_state_dict(checkpoint)\n",
        "        history = None\n",
        "\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    return model, history\n",
        "\n",
        "# Load all three models\n",
        "model_paths = {\n",
        "    'base': 'drive/MyDrive/results/archive21/base_model.pth',\n",
        "    'trivial_aug': 'drive/MyDrive/results/archive21/TA_model_3.pth',\n",
        "    'gans': 'drive/MyDrive/results/archive21/gan_model_2.pth',\n",
        "    'combined': 'drive/MyDrive/results/archive21/combined_model_2.pth'\n",
        "}\n",
        "\n",
        "models = {}\n",
        "histories = {}\n",
        "\n",
        "for model_name, path in model_paths.items():\n",
        "    try:\n",
        "        model, history = load_model_from_checkpoint(path, BrainCNN)\n",
        "        models[model_name] = model\n",
        "        histories[model_name] = history\n",
        "        print(f\"Successfully loaded {model_name} model\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {model_name} model: {str(e)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCuAyRUh7iM7",
        "outputId": "19abd1f9-025b-4095-c1f2-c3e6def6f3a2"
      },
      "id": "KCuAyRUh7iM7",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-9-e1b7da3716d2>:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(checkpoint_path, map_location=torch.device('cuda' if torch.cuda.is_available() else 'cpu'))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded base model\n",
            "Successfully loaded trivial_aug model\n",
            "Successfully loaded gans model\n",
            "Successfully loaded combined model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = models['base']\n",
        "ta_model = models['trivial_aug']\n",
        "gan_model = models['gans']\n",
        "combined_model = models['combined']"
      ],
      "metadata": {
        "id": "agapWZ7g7j5_"
      },
      "id": "agapWZ7g7j5_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "see misclassified :"
      ],
      "metadata": {
        "id": "jzHeWgshGcSV"
      },
      "id": "jzHeWgshGcSV"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_misclassified_indices(model, test_loader):\n",
        "    misclassified = []\n",
        "    base_idx = 0\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, (data, targets) in enumerate(test_loader):\n",
        "            outputs = model(data.to(next(model.parameters()).device))\n",
        "            predictions = outputs.argmax(dim=1)\n",
        "\n",
        "            # Get indices where predictions don't match targets\n",
        "            incorrect = predictions.cpu() != targets\n",
        "            batch_indices = incorrect.nonzero().squeeze().tolist()\n",
        "\n",
        "            # Convert batch indices to dataset indices\n",
        "            if isinstance(batch_indices, int):\n",
        "                batch_indices = [batch_indices]\n",
        "\n",
        "            dataset_indices = [base_idx + i for i in batch_indices]\n",
        "            misclassified.extend(dataset_indices)\n",
        "\n",
        "            base_idx += len(data)\n",
        "\n",
        "    return misclassified\n",
        "\n",
        "# Usage for each model\n",
        "misclassified_indices = {\n",
        "    model_name: get_misclassified_indices(model, test_loader_norm)\n",
        "    for model_name, model in models_dict.items()\n",
        "}"
      ],
      "metadata": {
        "id": "eQSdI4QoJ9mH"
      },
      "id": "eQSdI4QoJ9mH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "misclassified_indices"
      ],
      "metadata": {
        "id": "kQD_tUsJKKR7",
        "collapsed": true
      },
      "id": "kQD_tUsJKKR7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a id='gen'>Fidelity: LIME</a>"
      ],
      "metadata": {
        "id": "3yihh0wgGRcp"
      },
      "id": "3yihh0wgGRcp"
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_heatmap(heatmap):\n",
        "    # Convert tensor to numpy if needed\n",
        "    if isinstance(heatmap, torch.Tensor):\n",
        "        heatmap = heatmap.cpu().numpy()\n",
        "\n",
        "    # Ensure heatmap is 2D\n",
        "    if len(heatmap.shape) > 2:\n",
        "        # Check if channels are last dimension\n",
        "        if heatmap.shape[-1] in [3, 4]:\n",
        "            heatmap = np.mean(heatmap, axis=-1)\n",
        "        # Check if channels are first dimension\n",
        "        elif heatmap.shape[0] in [3, 4]:\n",
        "            heatmap = np.mean(heatmap, axis=0)\n",
        "\n",
        "    # Normalize to [0,1]\n",
        "    if heatmap.max() - heatmap.min() != 0:\n",
        "        heatmap = (heatmap - heatmap.min()) / (heatmap.max() - heatmap.min())\n",
        "\n",
        "    return heatmap\n",
        "\n",
        "def prepare_image(image, denormalize=True):\n",
        "    if torch.is_tensor(image):\n",
        "        image = image.cpu().numpy()\n",
        "\n",
        "    if image.shape[0] == 3:  # CHW to HWC\n",
        "        image = np.transpose(image, (1, 2, 0))\n",
        "\n",
        "    if denormalize:\n",
        "        mean = np.array([0.1948, 0.1948, 0.1948])\n",
        "        std = np.array([0.1768, 0.1768, 0.1768])\n",
        "        image = image * std + mean\n",
        "\n",
        "    return np.clip(image, 0, 1)\n",
        "\n",
        "from skimage.segmentation import felzenszwalb\n",
        "\n",
        "def get_specific_batch_images(loader, index):\n",
        "    \"\"\"Get specific image from DataLoader\"\"\"\n",
        "    for i, (images, labels) in enumerate(loader):\n",
        "        if i * loader.batch_size <= index < (i + 1) * loader.batch_size:\n",
        "            batch_idx = index - (i * loader.batch_size)\n",
        "            return images[batch_idx], labels[batch_idx]\n",
        "    return None, None\n",
        "\n",
        "def generate_lime_explanation(model, input_image, class_names, num_samples=1000):\n",
        "    \"\"\"Generate LIME explanations for a single input image.\"\"\"\n",
        "    def batch_predict(images):\n",
        "        model.eval()\n",
        "        batch = torch.stack([torch.from_numpy(i).permute(2, 0, 1).float() for i in images], dim=0)\n",
        "        device = next(model.parameters()).device\n",
        "        batch = batch.to(device)\n",
        "        with torch.no_grad():\n",
        "            logits = model(batch)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "        return probs.cpu().numpy()\n",
        "\n",
        "    if torch.is_tensor(input_image):\n",
        "        input_image = prepare_image(input_image, denormalize=True)\n",
        "\n",
        "    # First get the model's actual prediction\n",
        "    with torch.no_grad():\n",
        "        device = next(model.parameters()).device\n",
        "        input_tensor = torch.from_numpy(input_image.transpose(2, 0, 1)).float().unsqueeze(0).to(device)\n",
        "        logits = model(input_tensor)\n",
        "        probs = F.softmax(logits, dim=1)\n",
        "        pred_class = torch.argmax(probs[0]).item()\n",
        "        confidence = probs[0][pred_class].item() * 100\n",
        "\n",
        "    explainer = lime_image.LimeImageExplainer()\n",
        "    segmenter = SegmentationAlgorithm('felzenszwalb', scale=100, sigma=0.8, min_size=50)\n",
        "\n",
        "    explanation = explainer.explain_instance(\n",
        "        input_image,\n",
        "        batch_predict,\n",
        "        labels= len(class_names),\n",
        "        hide_color=0,\n",
        "        num_samples=num_samples,\n",
        "        segmentation_fn=segmenter\n",
        "    )\n",
        "\n",
        "    exp_img, mask = explanation.get_image_and_mask(\n",
        "        pred_class,\n",
        "        positive_only=False,\n",
        "        num_features=20,\n",
        "        hide_rest=False,\n",
        "        min_weight=0.01\n",
        "    )\n",
        "\n",
        "    mask = normalize_heatmap(mask)\n",
        "\n",
        "    return exp_img, mask, pred_class, confidence\n",
        "\n",
        "def visualize_results(results, original_image, class_names):\n",
        "    # Modified to only show LIME explanations\n",
        "    fig = plt.figure(figsize=(20, 4))\n",
        "    fig.suptitle('LIME Explanations Comparison', fontsize=16)\n",
        "\n",
        "    # Plot original image\n",
        "    ax = plt.subplot(1, 5, 1)\n",
        "    orig_img = prepare_image(original_image, denormalize=True)\n",
        "    ax.imshow(orig_img)\n",
        "    ax.set_title('Original Image')\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Plot LIME for each model\n",
        "    for idx, (model_name, result) in enumerate(results.items(), start=2):\n",
        "        pred_class = result.get('predicted_class', None)\n",
        "        pred_info = \"\"\n",
        "        if pred_class is not None and class_names is not None:\n",
        "            pred_info = f\"Pred: {class_names[pred_class]}\"\n",
        "            if 'probabilities' in result:\n",
        "                conf = result['probabilities'][pred_class] * 100\n",
        "                pred_info += f\"\\nConf: {conf:.1f}%\"\n",
        "\n",
        "        ax = plt.subplot(1, 5, idx)\n",
        "        ax.imshow(orig_img)\n",
        "        ax.imshow(result['lime'], cmap='RdYlBu_r', alpha=0.5)\n",
        "        ax.set_title(f'{model_name.capitalize()}\\n{pred_info}')\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def visualize_model_comparisons(models_dict, test_loader_norm, test_loader_unnorm, class_names, image_index=0):\n",
        "    \"\"\"\n",
        "    Visualize LIME explanations for all models and compare their predictions.\n",
        "    Handles cases where explanations for a label are not available.\n",
        "    \"\"\"\n",
        "    # Get images\n",
        "    image_norm, label = get_specific_batch_images(test_loader_norm, image_index)\n",
        "    image_unnorm, _ = get_specific_batch_images(test_loader_unnorm, image_index)\n",
        "\n",
        "    # Get predictions\n",
        "    direct_preds = {}\n",
        "    for model_name, model in models_dict.items():\n",
        "        with torch.no_grad():\n",
        "            device = next(model.parameters()).device\n",
        "            input_tensor = image_norm.unsqueeze(0).to(device)\n",
        "            logits = model(input_tensor)\n",
        "            probs = F.softmax(logits, dim=1)\n",
        "            pred_class = torch.argmax(probs[0]).item()\n",
        "            confidence = probs[0][pred_class].item() * 100\n",
        "            direct_preds[model_name] = {\n",
        "                'class': pred_class,\n",
        "                'confidence': confidence,\n",
        "                'probabilities': probs[0].cpu().numpy()\n",
        "            }\n",
        "\n",
        "    true_class = class_names[label]\n",
        "    print(f\"\\nTrue class: {true_class}\")\n",
        "    print(\"Model predictions:\")\n",
        "    for model_name, pred in direct_preds.items():\n",
        "        print(f\"{model_name}: {class_names[pred['class']]} ({pred['confidence']:.1f}%)\")\n",
        "\n",
        "    # Create visualization\n",
        "    fig = plt.figure(figsize=(20, 8))\n",
        "    fig.suptitle(f'LIME Explanations Comparison\\nTrue Class: {true_class}', fontsize=16)\n",
        "\n",
        "    # Plot original image\n",
        "    ax = plt.subplot(2, 5, 1)\n",
        "    orig_img = prepare_image(image_unnorm, denormalize=False)\n",
        "    ax.imshow(orig_img)\n",
        "    ax.set_title(f'Original Image\\nTrue: {true_class}')\n",
        "    ax.axis('off')\n",
        "\n",
        "    # Generate explanations for each model\n",
        "    print(\"\\nGenerating LIME explanations:\")\n",
        "    for idx, (model_name, model) in enumerate(models_dict.items(), start=1):\n",
        "        print(f\"\\nProcessing {model_name}...\")\n",
        "\n",
        "        pred_info = direct_preds[model_name]\n",
        "        pred_class = pred_info['class']\n",
        "        confidence = pred_info['confidence']\n",
        "\n",
        "        def batch_predict(images):\n",
        "            model.eval()\n",
        "            batch = torch.stack([torch.from_numpy(i).permute(2, 0, 1).float() for i in images], dim=0)\n",
        "            device = next(model.parameters()).device\n",
        "            batch = batch.to(device)\n",
        "            with torch.no_grad():\n",
        "                logits = model(batch)\n",
        "                probs = F.softmax(logits, dim=1)\n",
        "            return probs.cpu().numpy()\n",
        "\n",
        "        input_image = prepare_image(image_norm, denormalize=True)\n",
        "        explainer = lime_image.LimeImageExplainer()\n",
        "        segmenter = SegmentationAlgorithm('felzenszwalb', scale=100, sigma=0.8, min_size=50)\n",
        "\n",
        "        # Generate explanation for all classes\n",
        "        explanation = explainer.explain_instance(\n",
        "            input_image,\n",
        "            batch_predict,\n",
        "            top_labels=len(class_names),\n",
        "            hide_color=0,\n",
        "            num_samples=1000,\n",
        "            segmentation_fn=segmenter\n",
        "        )\n",
        "\n",
        "        # Plot explanation for predicted class (if available)\n",
        "        if pred_class in explanation.local_exp:\n",
        "            exp_img_pred, mask_pred = explanation.get_image_and_mask(\n",
        "                pred_class,\n",
        "                positive_only=False,\n",
        "                num_features=20,\n",
        "                hide_rest=False,\n",
        "                min_weight=0.01\n",
        "            )\n",
        "            mask_pred = normalize_heatmap(mask_pred)\n",
        "\n",
        "            ax = plt.subplot(2, 5, idx + 1)\n",
        "            ax.imshow(orig_img)\n",
        "            ax.imshow(mask_pred, cmap='RdBu', alpha=0.5)\n",
        "            ax.set_title(f'{model_name.capitalize()}\\nPredicted: {class_names[pred_class]}\\nConf: {confidence:.1f}%')\n",
        "            ax.axis('off')\n",
        "        else:\n",
        "            print(f\"Warning: No explanation found for predicted class {class_names[pred_class]} in {model_name}.\")\n",
        "\n",
        "        # Plot explanation for true class (if available)\n",
        "        if label in explanation.local_exp:\n",
        "            exp_img_true, mask_true = explanation.get_image_and_mask(\n",
        "                label,\n",
        "                positive_only=False,\n",
        "                num_features=20,\n",
        "                hide_rest=False,\n",
        "                min_weight=0.01\n",
        "            )\n",
        "            mask_true = normalize_heatmap(mask_true)\n",
        "\n",
        "            ax = plt.subplot(2, 5, idx + 6)\n",
        "            ax.imshow(orig_img)\n",
        "            ax.imshow(mask_true, cmap='RdBu', alpha=0.5)\n",
        "            ax.set_title(f'True Class Features\\n{true_class}')\n",
        "            ax.axis('off')\n",
        "        else:\n",
        "            print(f\"Warning: No explanation found for true class {true_class} in {model_name}.\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "nFpEi45XCi9X"
      },
      "id": "nFpEi45XCi9X",
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "zVp0V_wh4lDE"
      },
      "id": "zVp0V_wh4lDE"
    },
    {
      "cell_type": "code",
      "source": [
        "models_dict = {\n",
        "    'base': base_model,\n",
        "    'trivial_aug': ta_model,\n",
        "    'gans': gan_model,\n",
        "    'combined': combined_model\n",
        "}\n",
        "\n",
        "class_names = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
        "\n",
        "# Run visualization\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=400) #meningioma cases\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=415)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=425)"
      ],
      "metadata": {
        "id": "sxmzbqK3fbSJ",
        "collapsed": true
      },
      "id": "sxmzbqK3fbSJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run visualization\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=350) #meningioma cases\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=600)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=550)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7h7QHPijUAlS"
      },
      "id": "7h7QHPijUAlS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run visualization\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=192) #glioma\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=278)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=285)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wyru1uB8VP__"
      },
      "id": "wyru1uB8VP__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run visualization\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=68) #glioma\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=287)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=38)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "fKe6ekzqPqvM"
      },
      "id": "fKe6ekzqPqvM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run visualization\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=968) #no tumor\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=987)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=938)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "aTOW7Otd0dBl"
      },
      "id": "aTOW7Otd0dBl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run visualization\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1100) #pitu\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1200)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1195)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4F5p8OXETKdZ"
      },
      "id": "4F5p8OXETKdZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run visualization\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1211) #pitu\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1310)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1019)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Niz_dL9NT3Ge"
      },
      "id": "Niz_dL9NT3Ge",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "focus on misclassified:"
      ],
      "metadata": {
        "id": "T3jKQdM0Kcx0"
      },
      "id": "T3jKQdM0Kcx0"
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=30)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=44)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=45)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=50)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=229)"
      ],
      "metadata": {
        "id": "-Ph8200PKdie",
        "collapsed": true
      },
      "id": "-Ph8200PKdie",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=3)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=43)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=4)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=44)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1089)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "7Lyps3WNMdoh"
      },
      "id": "7Lyps3WNMdoh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "class-wise (base-focused):"
      ],
      "metadata": {
        "id": "FNe0VNAXODwl"
      },
      "id": "FNe0VNAXODwl"
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=702)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=635)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=701)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=714)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=843)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "FfNOgbGWOI-X"
      },
      "id": "FfNOgbGWOI-X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "miss-classified by combined:"
      ],
      "metadata": {
        "id": "c75HjdDzR5r4"
      },
      "id": "c75HjdDzR5r4"
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=159)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1043)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=224)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=356)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=548)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "O04oyAcDOC-m"
      },
      "id": "O04oyAcDOC-m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=242)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=361)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=583)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=1013)\n",
        "visualize_model_comparisons(models_dict, test_loader_norm, test_loader, class_names,  image_index=20)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "ZeN4yosoR9kI"
      },
      "id": "ZeN4yosoR9kI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Overall fidelity scores for LIME:"
      ],
      "metadata": {
        "id": "tIM9RbGj6tor"
      },
      "id": "tIM9RbGj6tor"
    },
    {
      "cell_type": "markdown",
      "source": [
        "for smaller set:"
      ],
      "metadata": {
        "id": "71IYMsHpxUTP"
      },
      "id": "71IYMsHpxUTP"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_class_fidelities(models_dict, test_loader_norm, class_names, num_samples=5):  # Reduced from 50 to 5\n",
        "    \"\"\"\n",
        "    Calculate average LIME fidelity scores for each model and class using fewer samples.\n",
        "\n",
        "    Args:\n",
        "        models_dict: Dictionary of models\n",
        "        test_loader_norm: Normalized test data loader\n",
        "        class_names: List of class names\n",
        "        num_samples: Number of images to evaluate per class (default reduced to 5)\n",
        "    \"\"\"\n",
        "    # Initialize storage for fidelity scores\n",
        "    fidelity_scores = {model_name: {class_name: [] for class_name in class_names}\n",
        "                      for model_name in models_dict.keys()}\n",
        "\n",
        "    # Get a limited number of images and labels\n",
        "    all_images = []\n",
        "    all_labels = []\n",
        "    max_images_per_class = num_samples + 2  # Add small buffer\n",
        "    class_counts = {i: 0 for i in range(len(class_names))}\n",
        "\n",
        "    for images, labels in test_loader_norm:\n",
        "        for img, label in zip(images, labels):\n",
        "            label_idx = label.item()\n",
        "            if class_counts[label_idx] < max_images_per_class:\n",
        "                all_images.append(img)\n",
        "                all_labels.append(label)\n",
        "                class_counts[label_idx] += 1\n",
        "\n",
        "        # Check if we have enough images for each class\n",
        "        if all(count >= max_images_per_class for count in class_counts.values()):\n",
        "            break\n",
        "\n",
        "    # Convert to numpy for easier indexing\n",
        "    all_labels = np.array([label.item() for label in all_labels])\n",
        "\n",
        "    # Process each class\n",
        "    for class_idx, class_name in enumerate(class_names):\n",
        "        print(f\"\\nProcessing class: {class_name}\")\n",
        "\n",
        "        # Get indices for this class\n",
        "        class_indices = np.where(all_labels == class_idx)[0]\n",
        "\n",
        "        # Sample images for this class\n",
        "        selected_indices = np.random.choice(class_indices,\n",
        "                                          size=min(num_samples, len(class_indices)),\n",
        "                                          replace=False)\n",
        "\n",
        "        # Process each selected image\n",
        "        for img_idx in selected_indices:\n",
        "            image = all_images[img_idx]\n",
        "\n",
        "            # Process each model\n",
        "            for model_name, model in models_dict.items():\n",
        "                print(f\"Processing model {model_name}, image {img_idx}\")\n",
        "\n",
        "                # Rest of the code remains the same...\n",
        "                def batch_predict(images):\n",
        "                    model.eval()\n",
        "                    batch = torch.stack([torch.from_numpy(i).permute(2, 0, 1).float() for i in images], dim=0)\n",
        "                    device = next(model.parameters()).device\n",
        "                    batch = batch.to(device)\n",
        "                    with torch.no_grad():\n",
        "                        logits = model(batch)\n",
        "                        probs = F.softmax(logits, dim=1)\n",
        "                    return probs.cpu().numpy()\n",
        "\n",
        "                input_image = prepare_image(image, denormalize=True)\n",
        "\n",
        "                # Get model's prediction\n",
        "                with torch.no_grad():\n",
        "                    device = next(model.parameters()).device\n",
        "                    input_tensor = image.unsqueeze(0).to(device)\n",
        "                    logits = model(input_tensor)\n",
        "                    probs = F.softmax(logits, dim=1)\n",
        "                    pred_class = torch.argmax(probs[0]).item()\n",
        "\n",
        "                # Generate LIME explanation\n",
        "                explainer = lime_image.LimeImageExplainer()\n",
        "                segmenter = SegmentationAlgorithm('felzenszwalb', scale=100, sigma=0.8, min_size=50)\n",
        "\n",
        "                explanation = explainer.explain_instance(\n",
        "                    input_image,\n",
        "                    batch_predict,\n",
        "                    top_labels=len(class_names),  # Modified to explain all classes\n",
        "                    hide_color=0,\n",
        "                    num_samples=1000,\n",
        "                    segmentation_fn=segmenter\n",
        "                )\n",
        "\n",
        "                # Store fidelity score\n",
        "                fidelity_scores[model_name][class_name].append(explanation.score)\n",
        "\n",
        "    # Calculate and display average fidelity scores\n",
        "    print(\"\\nAverage Fidelity Scores per Class:\")\n",
        "    for model_name in models_dict.keys():\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        for class_name in class_names:\n",
        "            scores = fidelity_scores[model_name][class_name]\n",
        "            avg_score = np.mean(scores) if scores else 0\n",
        "            std_score = np.std(scores) if scores else 0\n",
        "            print(f\"{class_name}: {avg_score:.3f} ± {std_score:.3f}\")\n",
        "\n",
        "    return fidelity_scores"
      ],
      "metadata": {
        "id": "mYgVzmW8xYTq"
      },
      "id": "mYgVzmW8xYTq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate fidelity scores with fewer samples\n",
        "fidelity_scores = calculate_class_fidelities(\n",
        "    models_dict=models_dict,\n",
        "    test_loader_norm=test_loader_norm,\n",
        "    class_names=class_names,\n",
        "    num_samples=5\n",
        ")\n",
        "\n",
        "# Save the scores\n",
        "import json\n",
        "with open('fidelity_scores.json', 'w') as f:\n",
        "    json.dump({k: {c: list(map(float, v)) for c, v in v.items()}\n",
        "              for k, v in fidelity_scores.items()}, f)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "1vs99FV1xTwA"
      },
      "id": "1vs99FV1xTwA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "larger set:"
      ],
      "metadata": {
        "id": "kSZPSoBiIbe1"
      },
      "id": "kSZPSoBiIbe1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate fidelity scores with fewer samples\n",
        "fidelity_scores = calculate_class_fidelities(\n",
        "    models_dict=models_dict,\n",
        "    test_loader_norm=test_loader_norm,\n",
        "    class_names=class_names,\n",
        "    num_samples=50\n",
        ")\n",
        "\n",
        "# Save the scores\n",
        "import json\n",
        "with open('fidelity_scores.json', 'w') as f:\n",
        "    json.dump({k: {c: list(map(float, v)) for c, v in v.items()}\n",
        "              for k, v in fidelity_scores.items()}, f)"
      ],
      "metadata": {
        "id": "L-3LOXItIYmb",
        "collapsed": true
      },
      "id": "L-3LOXItIYmb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a id='gen'>Sanity checks (Grad-CAM)</a>"
      ],
      "metadata": {
        "id": "z7d-r6RZx3ni"
      },
      "id": "z7d-r6RZx3ni"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SaveFeatures():\n",
        "    def __init__(self, module):\n",
        "        self.features = None\n",
        "        self.gradient = None\n",
        "        self.hook = module.register_forward_hook(self.hook_fn)\n",
        "\n",
        "    def hook_fn(self, module, input, output):\n",
        "        self.features = output\n",
        "\n",
        "    def remove(self):\n",
        "        self.hook.remove()\n",
        "\n",
        "def get_last_conv_layer(model):\n",
        "    \"\"\"Find the last convolutional layer in the model.\"\"\"\n",
        "    last_conv_layer = None\n",
        "    for module in model.features:\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            last_conv_layer = module\n",
        "    return last_conv_layer\n",
        "\n",
        "\n",
        "def generate_gradcam(model, image, target_class=None):\n",
        "    model.eval()\n",
        "    features = None\n",
        "\n",
        "    def save_features(module, input, output):\n",
        "        nonlocal features\n",
        "        features = output\n",
        "        features.retain_grad()\n",
        "\n",
        "    # Register hook on last conv layer\n",
        "    for module in reversed(model.features):\n",
        "        if isinstance(module, torch.nn.Conv2d):\n",
        "            handle = module.register_forward_hook(save_features)\n",
        "            break\n",
        "\n",
        "    # Forward pass\n",
        "    output = model(image.unsqueeze(0))\n",
        "\n",
        "    if target_class is None:\n",
        "        target_class = output.argmax(dim=1).item()\n",
        "\n",
        "    # Backward pass\n",
        "    model.zero_grad()\n",
        "    output[0, target_class].backward()\n",
        "\n",
        "    # Calculate Grad-CAM\n",
        "    pooled_grads = torch.mean(features.grad, dim=[0, 2, 3])\n",
        "    for i in range(features.shape[1]):\n",
        "        features[:, i, :, :] *= pooled_grads[i]\n",
        "\n",
        "    heatmap = torch.mean(features, dim=1).squeeze()\n",
        "    heatmap = F.relu(heatmap)\n",
        "    heatmap = (heatmap - heatmap.min()) / (heatmap.max() + 1e-8)\n",
        "\n",
        "    handle.remove()\n",
        "    return heatmap\n",
        "\n",
        "def compare_models_gradcam(models, image, class_names=None):\n",
        "    num_models = len(models)\n",
        "    plt.figure(figsize=(5*num_models, 5))\n",
        "\n",
        "    for idx, (name, model) in enumerate(models.items(), 1):\n",
        "        plt.subplot(1, num_models, idx)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(image.unsqueeze(0)).argmax().item()\n",
        "\n",
        "        heatmap = generate_gradcam(model, image).detach().cpu().numpy()\n",
        "        image_np = image.permute(1, 2, 0).cpu().numpy()\n",
        "        image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())\n",
        "\n",
        "        heatmap = transforms.functional.resize(\n",
        "            torch.from_numpy(heatmap).unsqueeze(0).unsqueeze(0),\n",
        "            image_np.shape[:2]\n",
        "        ).squeeze().numpy()\n",
        "\n",
        "        plt.imshow(image_np)\n",
        "        plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "        title = f\"{name}\\n\"\n",
        "        if class_names:\n",
        "            title += f\"Pred: {class_names[pred]}\"\n",
        "        plt.title(title)\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "models_dict = {\n",
        "    'base': base_model,\n",
        "    'trivial_aug': ta_model,\n",
        "    'gans': gan_model,\n",
        "    'combined': combined_model\n",
        "}"
      ],
      "metadata": {
        "id": "kpkuUwyhyNrG"
      },
      "id": "kpkuUwyhyNrG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get a sample image from test loader\n",
        "images, labels = next(iter(test_loader_norm))\n",
        "image = images[25]\n",
        "\n",
        "# Compare Grad-CAM across all models\n",
        "compare_models_gradcam(models_dict, image)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "CjT_aYHRkpQX"
      },
      "id": "CjT_aYHRkpQX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "with predictions:"
      ],
      "metadata": {
        "id": "gMpqyOxNk2QX"
      },
      "id": "gMpqyOxNk2QX"
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class_names = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
        "\n",
        "def show_gradcam_with_predictions(models_dict, dataset, index=0, device='cuda'):\n",
        "    \"\"\"\n",
        "    Show GradCAM visualization for a specific image index from the dataset.\n",
        "\n",
        "    Args:\n",
        "        models_dict (dict): Dictionary of models to generate GradCAM for\n",
        "        dataset: The full dataset (not DataLoader)\n",
        "        index (int): Index of the image to visualize\n",
        "        device (str): Device to run the models on ('cuda' or 'cpu')\n",
        "    \"\"\"\n",
        "    # Create a single-item subset and dataloader\n",
        "    single_item_subset = Subset(dataset, [index])\n",
        "    single_loader = DataLoader(single_item_subset, batch_size=1, shuffle=False)\n",
        "\n",
        "    # Get the specific image\n",
        "    image, label = next(iter(single_loader))\n",
        "    image = image[0].to(device)  # Remove batch dimension and move to device\n",
        "    true_label = label[0].item()\n",
        "\n",
        "    num_plots = len(models_dict) + 1  # +1 for original image\n",
        "    plt.figure(figsize=(5*num_plots, 5))\n",
        "\n",
        "    # Original image\n",
        "    plt.subplot(1, num_plots, 1)\n",
        "    image_np = image.cpu().permute(1, 2, 0).numpy()\n",
        "    image_np = (image_np - image_np.min()) / (image_np.max() - image_np.min())\n",
        "    plt.imshow(image_np)\n",
        "    plt.title(f\"Original Image\\nTrue Class: {class_names[true_label]}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    # GradCAM for each model\n",
        "    for idx, (name, model) in enumerate(models_dict.items(), 2):\n",
        "        plt.subplot(1, num_plots, idx)\n",
        "\n",
        "        # Create a deep copy of the model for GradCAM\n",
        "        model_copy = copy.deepcopy(model)\n",
        "        model_copy.eval()\n",
        "        model_copy.to(device)\n",
        "\n",
        "        # Get prediction from original model\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            pred = model(image.unsqueeze(0))\n",
        "            pred_class = pred.argmax().item()\n",
        "            confidence = torch.softmax(pred, dim=1)[0, pred_class].item()\n",
        "\n",
        "        # Generate GradCAM using the copy\n",
        "        heatmap = generate_gradcam(model_copy, image.clone()).detach().cpu().numpy()\n",
        "\n",
        "        # Clear memory\n",
        "        del model_copy\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        # Resize heatmap to match image size\n",
        "        heatmap = transforms.functional.resize(\n",
        "            torch.from_numpy(heatmap).unsqueeze(0).unsqueeze(0),\n",
        "            image_np.shape[:2]\n",
        "        ).squeeze().numpy()\n",
        "\n",
        "        plt.imshow(image_np)\n",
        "        plt.imshow(heatmap, cmap='jet', alpha=0.5)\n",
        "        plt.title(f\"{name}\\nPred: {class_names[pred_class]}\\nConf: {confidence:.2%}\")\n",
        "        plt.axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "MG6OD4X-lo3s"
      },
      "id": "MG6OD4X-lo3s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "misclassified examples:"
      ],
      "metadata": {
        "id": "-_-yPY-tul4a"
      },
      "id": "-_-yPY-tul4a"
    },
    {
      "cell_type": "code",
      "source": [
        "device='cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "IwbgH4xjzKHv"
      },
      "id": "IwbgH4xjzKHv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the underlying dataset instead:\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=30, device=device)\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=44, device=device)\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=45, device=device)\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=50, device=device)\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=229, device=device)\n"
      ],
      "metadata": {
        "id": "-_XEag8ek7hg",
        "collapsed": true
      },
      "id": "-_XEag8ek7hg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=400, device=device)\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=300, device=device)\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=500, device=device)\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=1000, device=device)\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=1100, device=device)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "5zXIx-vFuotS"
      },
      "id": "5zXIx-vFuotS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=242, device=device)\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=361, device=device)\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=583, device=device)\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=1013, device=device)\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=20, device=device)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "wkig69dGvlOY"
      },
      "id": "wkig69dGvlOY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=400, device=device) #menings\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=415, device=device)\n",
        "show_gradcam_with_predictions(models_dict, test_dataset_norm, index=425, device=device)"
      ],
      "metadata": {
        "id": "iYEyScqBwMxw"
      },
      "id": "iYEyScqBwMxw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_gradcam_activation(heatmap, image):\n",
        "    \"\"\"\n",
        "    Analyze the Grad-CAM activation map characteristics\n",
        "    \"\"\"\n",
        "    # Calculate activation statistics\n",
        "    activation_mean = np.mean(heatmap)\n",
        "    activation_std = np.std(heatmap)\n",
        "\n",
        "    # Calculate coverage (percentage of image with significant activation)\n",
        "    significant_threshold = 0.5  # Threshold for significant activation\n",
        "    coverage = np.mean(heatmap > significant_threshold)\n",
        "\n",
        "    # Calculate focus (ratio of max activation to mean activation)\n",
        "    focus_ratio = np.max(heatmap) / (activation_mean + 1e-6)\n",
        "\n",
        "    return {\n",
        "        'mean_activation': float(activation_mean),\n",
        "        'activation_std': float(activation_std),\n",
        "        'coverage': float(coverage),\n",
        "        'focus_ratio': float(focus_ratio)\n",
        "    }\n",
        "\n",
        "def summarize_gradcam_results(models_dict, test_loader, class_names=None):\n",
        "    \"\"\"\n",
        "    Generate a comprehensive summary of Grad-CAM results across models for the entire test set\n",
        "    \"\"\"\n",
        "    summary = {model_name: {'correct': 0, 'stats': []} for model_name in models_dict.keys()}\n",
        "    total_samples = len(test_loader.dataset)\n",
        "\n",
        "    print(f\"Analyzing {total_samples} images...\")\n",
        "\n",
        "    # Process all batches\n",
        "    for images, labels in tqdm(test_loader, desc=\"Processing batches\"):\n",
        "        for i in range(len(images)):\n",
        "            image = images[i]\n",
        "            true_label = labels[i].item()\n",
        "\n",
        "            for name, model in models_dict.items():\n",
        "                # Get prediction\n",
        "                with torch.no_grad():\n",
        "                    pred = model(image.unsqueeze(0))\n",
        "                    pred_class = pred.argmax().item()\n",
        "                    confidence = torch.softmax(pred, dim=1)[0, pred_class].item()\n",
        "\n",
        "                # Generate and analyze Grad-CAM\n",
        "                heatmap = generate_gradcam(model, image).detach().cpu().numpy()\n",
        "                heatmap = transforms.functional.resize(\n",
        "                    torch.from_numpy(heatmap).unsqueeze(0).unsqueeze(0),\n",
        "                    image.shape[1:]\n",
        "                ).squeeze().numpy()\n",
        "\n",
        "                # Analyze activation\n",
        "                stats = analyze_gradcam_activation(heatmap, image)\n",
        "                stats['confidence'] = confidence\n",
        "                stats['correct_prediction'] = (pred_class == true_label)\n",
        "\n",
        "                # Update summary\n",
        "                summary[name]['stats'].append(stats)\n",
        "                if pred_class == true_label:\n",
        "                    summary[name]['correct'] += 1\n",
        "\n",
        "    return format_gradcam_summary(summary, total_samples, class_names)\n",
        "\n",
        "def format_gradcam_summary(summary, total_samples, class_names):\n",
        "    \"\"\"\n",
        "    Format the Grad-CAM analysis results into a readable report\n",
        "    \"\"\"\n",
        "    report = \"Grad-CAM Analysis Summary\\n\" + \"=\"*50 + \"\\n\\n\"\n",
        "    report += f\"Total samples analyzed: {total_samples}\\n\\n\"\n",
        "\n",
        "    for model_name, model_data in summary.items():\n",
        "        report += f\"\\nModel: {model_name}\\n{'-'*30}\\n\"\n",
        "\n",
        "        # Accuracy\n",
        "        accuracy = model_data['correct'] / total_samples\n",
        "        report += f\"Accuracy: {accuracy:.2%}\\n\"\n",
        "\n",
        "        # Average statistics\n",
        "        stats = model_data['stats']\n",
        "        avg_stats = {\n",
        "            'mean_activation': np.mean([s['mean_activation'] for s in stats]),\n",
        "            'activation_std': np.mean([s['activation_std'] for s in stats]),\n",
        "            'coverage': np.mean([s['coverage'] for s in stats]),\n",
        "            'focus_ratio': np.mean([s['focus_ratio'] for s in stats]),\n",
        "            'confidence': np.mean([s['confidence'] for s in stats])\n",
        "        }\n",
        "\n",
        "        report += f\"Average Activation: {avg_stats['mean_activation']:.3f}\\n\"\n",
        "        report += f\"Average Coverage: {avg_stats['coverage']:.2%}\\n\"\n",
        "        report += f\"Focus Ratio: {avg_stats['focus_ratio']:.2f}\\n\"\n",
        "        report += f\"Average Confidence: {avg_stats['confidence']:.2%}\\n\"\n",
        "\n",
        "        # Standard deviations\n",
        "        std_stats = {\n",
        "            'activation_std': np.std([s['mean_activation'] for s in stats]),\n",
        "            'coverage_std': np.std([s['coverage'] for s in stats]),\n",
        "            'focus_ratio_std': np.std([s['focus_ratio'] for s in stats]),\n",
        "            'confidence_std': np.std([s['confidence'] for s in stats])\n",
        "        }\n",
        "\n",
        "        report += f\"\\nVariability Analysis:\\n\"\n",
        "        report += f\"Activation Std: {std_stats['activation_std']:.3f}\\n\"\n",
        "        report += f\"Coverage Std: {std_stats['coverage_std']:.2%}\\n\"\n",
        "        report += f\"Focus Ratio Std: {std_stats['focus_ratio_std']:.2f}\\n\"\n",
        "        report += f\"Confidence Std: {std_stats['confidence_std']:.2%}\\n\"\n",
        "\n",
        "        # Analyze activation patterns\n",
        "        report += \"\\nActivation Analysis:\\n\"\n",
        "        if avg_stats['coverage'] > 0.7:\n",
        "            report += \"- Wide activation pattern (might be looking at too much)\\n\"\n",
        "        elif avg_stats['coverage'] < 0.3:\n",
        "            report += \"- Focused activation pattern (concentrated attention)\\n\"\n",
        "\n",
        "        if avg_stats['focus_ratio'] > 5:\n",
        "            report += \"- High focus ratio (very specific features)\\n\"\n",
        "        elif avg_stats['focus_ratio'] < 2:\n",
        "            report += \"- Low focus ratio (more distributed attention)\\n\"\n",
        "\n",
        "        # Add histogram visualization of key metrics\n",
        "        plt.figure(figsize=(15, 10))\n",
        "        plt.subplot(2, 2, 1)\n",
        "        plt.hist([s['mean_activation'] for s in stats], bins=30)\n",
        "        plt.title('Distribution of Mean Activation')\n",
        "\n",
        "        plt.subplot(2, 2, 2)\n",
        "        plt.hist([s['coverage'] for s in stats], bins=30)\n",
        "        plt.title('Distribution of Coverage')\n",
        "\n",
        "        plt.subplot(2, 2, 3)\n",
        "        plt.hist([s['focus_ratio'] for s in stats], bins=30)\n",
        "        plt.title('Distribution of Focus Ratio')\n",
        "\n",
        "        plt.subplot(2, 2, 4)\n",
        "        plt.hist([s['confidence'] for s in stats], bins=30)\n",
        "        plt.title('Distribution of Confidence')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return report\n",
        "\n",
        "# Example usage:\n",
        "def run_gradcam_analysis(models_dict, test_loader, class_names):\n",
        "    # Run analysis on entire test set\n",
        "    summary = summarize_gradcam_results(\n",
        "        models_dict,\n",
        "        test_loader,\n",
        "        class_names=class_names\n",
        "    )\n",
        "    print(summary)"
      ],
      "metadata": {
        "id": "iKE0w3Atk5Rk"
      },
      "id": "iKE0w3Atk5Rk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_gradcam_analysis(models_dict, test_loader_norm, class_names)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "sF50Wczlk7gl"
      },
      "id": "sF50Wczlk7gl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SSIM"
      ],
      "metadata": {
        "id": "GXrc-ECdZwf9"
      },
      "id": "GXrc-ECdZwf9"
    },
    {
      "cell_type": "code",
      "source": [
        "def randomize_layer_weights(model, num_layers):\n",
        "    \"\"\"Randomize weights of last n layers\"\"\"\n",
        "    params = list(model.parameters())\n",
        "    for i in range(min(num_layers * 2, len(params))):  # *2 because each layer has weights and bias\n",
        "        original_weights = params[-(i+1)].data.clone()\n",
        "        params[-(i+1)].data = torch.randn_like(original_weights)\n",
        "    return original_weights, -(i+1)\n",
        "\n",
        "def reset_layer_weights(model, weights, layer_idx):\n",
        "    \"\"\"Reset weights of a layer back to original\"\"\"\n",
        "    params = list(model.parameters())\n",
        "    params[layer_idx].data = weights\n",
        "\n",
        "def calculate_ssim_masks_batch(models_dict, test_loader, num_layers=6, num_samples=None):\n",
        "    \"\"\"\n",
        "    Calculate SSIM scores for multiple images\n",
        "\n",
        "    Args:\n",
        "        models_dict: Dictionary of models to analyze\n",
        "        test_loader: DataLoader containing test images\n",
        "        num_layers: Number of layers to progressively randomize\n",
        "        num_samples: Optional number of samples to analyze (None for all)\n",
        "    \"\"\"\n",
        "    # Initialize storage for SSIM scores\n",
        "    ssim_scores = {name: [] for name in models_dict.keys()}\n",
        "    x_labels = ['original', 'classifier', 'denseblock4', 'denseblock3',\n",
        "                'denseblock2', 'denseblock1'][:num_layers+1]\n",
        "\n",
        "    # Count total samples to process\n",
        "    total_samples = len(test_loader.dataset) if num_samples is None else num_samples\n",
        "    samples_processed = 0\n",
        "\n",
        "    # Process each batch\n",
        "    for images, _ in tqdm(test_loader, desc=\"Processing images\"):\n",
        "        batch_scores = {name: [] for name in models_dict.keys()}\n",
        "\n",
        "        # Process each image in the batch\n",
        "        for image in images:\n",
        "            if samples_processed >= total_samples and num_samples is not None:\n",
        "                break\n",
        "\n",
        "            for name, model in models_dict.items():\n",
        "                # Generate original mask\n",
        "                original_mask = generate_gradcam(model, image).detach().cpu().numpy()\n",
        "                current_scores = [1.0]  # SSIM with itself = 1\n",
        "\n",
        "                # Store original weights for reset\n",
        "                original_weights = []\n",
        "                layer_indices = []\n",
        "\n",
        "                # Progressive randomization\n",
        "                for layer in range(num_layers):\n",
        "                    weights, idx = randomize_layer_weights(model, layer + 1)\n",
        "                    original_weights.append(weights)\n",
        "                    layer_indices.append(idx)\n",
        "\n",
        "                    # Generate new mask and calculate SSIM\n",
        "                    random_mask = generate_gradcam(model, image).detach().cpu().numpy()\n",
        "                    score = ssim(original_mask, random_mask, data_range=1.0)\n",
        "                    current_scores.append(score)\n",
        "\n",
        "                    # Reset weights for next iteration\n",
        "                    for w, i in zip(original_weights[:-1], layer_indices[:-1]):\n",
        "                        reset_layer_weights(model, w, i)\n",
        "\n",
        "                batch_scores[name].append(current_scores)\n",
        "\n",
        "            samples_processed += 1\n",
        "\n",
        "        # Aggregate batch results\n",
        "        for name in models_dict.keys():\n",
        "            if not ssim_scores[name]:  # First batch\n",
        "                ssim_scores[name] = [[] for _ in range(num_layers + 1)]\n",
        "            for scores in batch_scores[name]:\n",
        "                for i, score in enumerate(scores):\n",
        "                    ssim_scores[name][i].append(score)\n",
        "\n",
        "    # Calculate statistics\n",
        "    ssim_stats = {name: {\n",
        "        'mean': [np.mean(layer_scores) for layer_scores in model_scores],\n",
        "        'std': [np.std(layer_scores) for layer_scores in model_scores]\n",
        "    } for name, model_scores in ssim_scores.items()}\n",
        "\n",
        "    # Plot results with error bars\n",
        "    plt.figure(figsize=(12, 7))\n",
        "    for name, stats in ssim_stats.items():\n",
        "        plt.errorbar(range(len(stats['mean'])),\n",
        "                    stats['mean'],\n",
        "                    yerr=stats['std'],\n",
        "                    marker='o',\n",
        "                    label=f'{name}',\n",
        "                    capsize=5)\n",
        "\n",
        "    plt.xticks(range(len(x_labels)), x_labels, rotation=45)\n",
        "    plt.ylabel('SSIM score (mean ± std)')\n",
        "    plt.xlabel('Randomized layers')\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.title(f'Sanity Check: SSIM Similarity vs. Layer Randomization\\n(n={samples_processed} images)')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return ssim_stats\n",
        "\n",
        "# Example usage:\n",
        "def run_ssim_analysis(models_dict, test_loader, num_samples=None):\n",
        "    \"\"\"\n",
        "    Run SSIM analysis on multiple images\n",
        "\n",
        "    Args:\n",
        "        models_dict: Dictionary of models to analyze\n",
        "        test_loader: DataLoader containing test images\n",
        "        num_samples: Optional number of samples to analyze (None for all)\n",
        "    \"\"\"\n",
        "    stats = calculate_ssim_masks_batch(\n",
        "        models_dict,\n",
        "        test_loader,\n",
        "        num_layers=6,\n",
        "        num_samples=num_samples\n",
        "    )\n",
        "    return stats"
      ],
      "metadata": {
        "id": "yh_sEheylU45"
      },
      "id": "yh_sEheylU45",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stats = run_ssim_analysis(models_dict, test_loader_norm, num_samples=100)"
      ],
      "metadata": {
        "id": "bZB3YDs2lX5O"
      },
      "id": "bZB3YDs2lX5O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a id='gen'>Sparsity using shap: </a>\n",
        "\n",
        "(clean the backward hook from grad-cam)"
      ],
      "metadata": {
        "id": "hk2vumSeoJMo"
      },
      "id": "hk2vumSeoJMo"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_shap_sparsity(models_dict, test_loader_norm, class_names, num_samples=15):\n",
        "    \"\"\"Calculate SHAP sparsity scores for each model and class with optimized performance.\n",
        "\n",
        "    Args:\n",
        "        models_dict (dict): Dictionary of model name to model\n",
        "        test_loader_norm (DataLoader): Test data loader\n",
        "        class_names (list): List of class names\n",
        "        num_samples (int): Number of samples per class to analyze\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    sparsity_scores = {model_name: {class_name: [] for class_name in class_names}\n",
        "                      for model_name in models_dict.keys()}\n",
        "\n",
        "    # Collect samples per class\n",
        "    print(\"Collecting samples...\")\n",
        "    class_samples = {class_name: [] for class_name in class_names}\n",
        "    for images, labels in test_loader_norm:\n",
        "        for img, label in zip(images, labels):\n",
        "            class_name = class_names[label.item()]\n",
        "            if len(class_samples[class_name]) < num_samples:\n",
        "                class_samples[class_name].append(img)\n",
        "\n",
        "        # Check if we have enough samples\n",
        "        if all(len(samples) >= num_samples for samples in class_samples.values()):\n",
        "            break\n",
        "\n",
        "    # Process each model\n",
        "    for model_name, model in models_dict.items():\n",
        "        print(f\"\\nProcessing model: {model_name}\")\n",
        "        model.eval()\n",
        "\n",
        "        # Move model to GPU if available\n",
        "        if torch.cuda.is_available():\n",
        "            model = model.cuda()\n",
        "\n",
        "        # Process each class\n",
        "        for class_name in class_names:\n",
        "            print(f\"\\nProcessing class: {class_name}\")\n",
        "            samples = class_samples[class_name][:num_samples]\n",
        "\n",
        "            if not samples:\n",
        "                continue\n",
        "\n",
        "            # Process in small batches\n",
        "            batch_size = 5\n",
        "            for i in range(0, len(samples), batch_size):\n",
        "                batch = samples[i:i+batch_size]\n",
        "                batch = torch.stack(batch)\n",
        "\n",
        "                if torch.cuda.is_available():\n",
        "                    batch = batch.cuda()\n",
        "\n",
        "                try:\n",
        "                    # Create explainer for this batch\n",
        "                    background = torch.zeros_like(batch[0:1])\n",
        "                    explainer = shap.GradientExplainer(model, background)\n",
        "\n",
        "                    # Get SHAP values\n",
        "                    shap_values = explainer.shap_values(batch)\n",
        "\n",
        "                    # Process each image in batch\n",
        "                    for idx in range(len(batch)):\n",
        "                        if isinstance(shap_values, list):\n",
        "                            # For multi-class output, use predicted class\n",
        "                            with torch.no_grad():\n",
        "                                pred_class = model(batch[idx:idx+1]).argmax().item()\n",
        "                            shap_map = np.abs(shap_values[pred_class][idx])\n",
        "                        else:\n",
        "                            shap_map = np.abs(shap_values[idx])\n",
        "\n",
        "                        # Calculate sparsity with 5% threshold\n",
        "                        threshold = np.max(shap_map) * 0.05\n",
        "                        sparsity = np.mean(shap_map < threshold)\n",
        "                        sparsity_scores[model_name][class_name].append(sparsity)\n",
        "\n",
        "                        print(f\"Image {i+idx+1}/{len(samples)}: Sparsity = {sparsity:.3f}\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing batch: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "                finally:\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nFinal Sparsity Scores:\")\n",
        "    for model_name in models_dict.keys():\n",
        "        print(f\"\\n{model_name}:\")\n",
        "        for class_name in class_names:\n",
        "            scores = sparsity_scores[model_name][class_name]\n",
        "            if scores:\n",
        "                avg_score = np.mean(scores)\n",
        "                std_score = np.std(scores)\n",
        "                print(f\"{class_name}: {avg_score:.3f} ± {std_score:.3f} (n={len(scores)})\")\n",
        "            else:\n",
        "                print(f\"{class_name}: No valid scores\")\n",
        "\n",
        "    return sparsity_scores"
      ],
      "metadata": {
        "id": "51ihAJnf2fat"
      },
      "id": "51ihAJnf2fat",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sparsity_scores = calculate_shap_sparsity(models_dict, test_loader_norm, class_names)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "6PFiSZEV2hoC"
      },
      "id": "6PFiSZEV2hoC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# <a id='gen'>Class-wise contrastivity - using shap: </a>"
      ],
      "metadata": {
        "id": "HWWeZlaXr2WK"
      },
      "id": "HWWeZlaXr2WK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "try on small set:"
      ],
      "metadata": {
        "id": "goyeRDS_-jed"
      },
      "id": "goyeRDS_-jed"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_class_contrast(models_dict, test_loader_norm, class_names, num_samples=10):  # Reduced to 2 samples\n",
        "    \"\"\"\n",
        "    Calculate and visualize class-wise SHAP contrast for each model.\n",
        "    Debug version with minimal samples.\n",
        "    \"\"\"\n",
        "    class_contrasts = {}\n",
        "\n",
        "    # Get all images and labels\n",
        "    print(\"\\nCollecting samples...\")\n",
        "    all_images = {class_name: [] for class_name in class_names}\n",
        "    all_labels = {class_name: [] for class_name in class_names}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader_norm:\n",
        "            for img, label in zip(images, labels):\n",
        "                class_name = class_names[label.item()]\n",
        "                if len(all_images[class_name]) < num_samples:\n",
        "                    all_images[class_name].append(img.cpu())\n",
        "                    all_labels[class_name].append(label.cpu())\n",
        "\n",
        "            # Check if we have enough samples for all classes\n",
        "            if all(len(imgs) >= num_samples for imgs in all_images.values()):\n",
        "                break\n",
        "\n",
        "    # Print collection results\n",
        "    for class_name in class_names:\n",
        "        print(f\"Collected {len(all_images[class_name])} images for {class_name}\")\n",
        "\n",
        "    if any(len(imgs) < num_samples for imgs in all_images.values()):\n",
        "        print(\"Warning: Could not collect enough samples for all classes\")\n",
        "\n",
        "    for model_name, model in models_dict.items():\n",
        "        print(f\"\\nProcessing model: {model_name}\")\n",
        "        class_shap_values = {class_name: [] for class_name in class_names}\n",
        "\n",
        "        # Set model to eval mode\n",
        "        model.eval()\n",
        "\n",
        "        # Process each class\n",
        "        for class_name in class_names:\n",
        "            print(f\"\\nProcessing class: {class_name}\")\n",
        "            if not all_images[class_name]:\n",
        "                print(f\"Warning: No samples found for class {class_name}\")\n",
        "                continue\n",
        "\n",
        "            for i, image in enumerate(all_images[class_name][:num_samples]):\n",
        "                try:\n",
        "                    print(f\"\\nProcessing image {i+1}/{num_samples}\")\n",
        "                    # Move image to same device as model\n",
        "                    device = next(model.parameters()).device\n",
        "                    image = image.to(device)\n",
        "\n",
        "                    # Create background with zeros\n",
        "                    background = torch.zeros_like(image.unsqueeze(0))\n",
        "                    explainer = shap.GradientExplainer(model, background)\n",
        "\n",
        "                    # Get SHAP values for the actual image\n",
        "                    shap_values = explainer.shap_values(image.unsqueeze(0))\n",
        "                    print(f\"Raw SHAP values type: {type(shap_values)}\")\n",
        "                    print(f\"Raw SHAP values shape/length: {len(shap_values) if isinstance(shap_values, list) else shap_values.shape}\")\n",
        "\n",
        "                    # Process SHAP values\n",
        "                    if isinstance(shap_values, list):\n",
        "                        shap_map = np.mean(np.abs(shap_values[0][0]), axis=(0, -1))  # Average across classes and channels\n",
        "                    else:\n",
        "                        # Shape is (1, 3, 128, 128, 4)\n",
        "                        shap_map = np.mean(np.abs(shap_values[0]), axis=(0, -1))  # Average across classes and channels\n",
        "\n",
        "                    print(f\"Processed SHAP map shape: {shap_map.shape}\")\n",
        "                    print(f\"SHAP values range: [{np.min(shap_map):.3f}, {np.max(shap_map):.3f}]\")\n",
        "\n",
        "                    class_shap_values[class_name].append(shap_map)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image {img_idx}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        # Calculate average SHAP values per class\n",
        "        avg_shap_values = {\n",
        "            class_name: np.mean(values, axis=0) if values else None\n",
        "            for class_name, values in class_shap_values.items()\n",
        "        }\n",
        "\n",
        "        # Print average values info\n",
        "        for class_name, avg_val in avg_shap_values.items():\n",
        "            if avg_val is not None:\n",
        "                print(f\"\\nAverage SHAP values for {class_name}:\")\n",
        "                print(f\"Shape: {avg_val.shape}\")\n",
        "                print(f\"Range: [{np.min(avg_val):.3f}, {np.max(avg_val):.3f}]\")\n",
        "\n",
        "        # Visualize class contrasts\n",
        "        num_comparisons = len(list(itertools.combinations(class_names, 2)))\n",
        "        if num_comparisons > 0:\n",
        "            plt.figure(figsize=(15, 5*((num_comparisons+1)//2)))\n",
        "\n",
        "            for i, (class1, class2) in enumerate(itertools.combinations(class_names, 2)):\n",
        "                if avg_shap_values[class1] is None or avg_shap_values[class2] is None:\n",
        "                    print(f\"\\nSkipping {class1} vs {class2} - missing values\")\n",
        "                    continue\n",
        "\n",
        "                contrast = avg_shap_values[class1] - avg_shap_values[class2]\n",
        "                print(f\"\\nContrast shape for {class1} vs {class2}: {contrast.shape}\")\n",
        "                print(f\"Contrast range: [{np.min(contrast):.3f}, {np.max(contrast):.3f}]\")\n",
        "\n",
        "                # Handle normalization carefully\n",
        "                vmax = np.max(np.abs(contrast))\n",
        "                if vmax < 1e-6:\n",
        "                    print(f\"Warning: Very small contrast between {class1} and {class2}\")\n",
        "                    continue\n",
        "\n",
        "                norm = TwoSlopeNorm(vmin=-vmax, vcenter=0, vmax=vmax)\n",
        "\n",
        "                plt.subplot((num_comparisons+1)//2, 2, i+1)\n",
        "                plt.imshow(contrast, cmap='RdBu_r', norm=norm)\n",
        "                plt.title(f'{class1} vs {class2}')\n",
        "                plt.colorbar()\n",
        "\n",
        "            plt.suptitle(f'Class Contrasts - {model_name}')\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Warning: Not enough classes for comparison\")\n",
        "\n",
        "        class_contrasts[model_name] = avg_shap_values\n",
        "\n",
        "    return class_contrasts"
      ],
      "metadata": {
        "id": "emiK_Z0O-jMq"
      },
      "id": "emiK_Z0O-jMq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contrasts = calculate_class_contrast(models_dict, test_loader_norm, class_names) #10 samples"
      ],
      "metadata": {
        "id": "oz_zR4wbdFM7",
        "collapsed": true
      },
      "id": "oz_zR4wbdFM7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contrasts = calculate_class_contrast(models_dict, test_loader_norm, class_names)"
      ],
      "metadata": {
        "id": "VMmBrhsL-n0B",
        "collapsed": true
      },
      "id": "VMmBrhsL-n0B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "new version:"
      ],
      "metadata": {
        "id": "gu1on-B15Bx3"
      },
      "id": "gu1on-B15Bx3"
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_class_contrast_matrix(models_dict, test_loader_norm, class_names, num_samples=10):\n",
        "    \"\"\"\n",
        "    Calculate class-wise contrastivity matrix using SHAP values.\n",
        "    Returns a matrix of contrastivity scores between each pair of classes.\n",
        "    \"\"\"\n",
        "    class_contrasts = {}\n",
        "\n",
        "    # Get all images and labels\n",
        "    all_images = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader_norm:\n",
        "            all_images.extend(images.cpu())\n",
        "            all_labels.extend(labels.cpu())\n",
        "    all_labels = np.array([label.item() for label in all_labels])\n",
        "\n",
        "    for model_name, model in models_dict.items():\n",
        "        print(f\"\\nProcessing model: {model_name}\")\n",
        "        class_shap_values = {class_name: [] for class_name in class_names}\n",
        "\n",
        "        # Set model to eval mode\n",
        "        model.eval()\n",
        "\n",
        "        # Process each class\n",
        "        for class_idx, class_name in enumerate(class_names):\n",
        "            class_indices = np.where(all_labels == class_idx)[0]\n",
        "            selected_indices = np.random.choice(class_indices,\n",
        "                                             size=min(num_samples, len(class_indices)),\n",
        "                                             replace=False)\n",
        "\n",
        "            for img_idx in selected_indices:\n",
        "                image = all_images[img_idx].to(next(model.parameters()).device)\n",
        "\n",
        "                try:\n",
        "                    # Get SHAP values\n",
        "                    background = torch.zeros_like(image.unsqueeze(0))\n",
        "                    explainer = shap.GradientExplainer(model, background)\n",
        "                    shap_values = explainer.shap_values(image.unsqueeze(0))\n",
        "\n",
        "                    # Process SHAP values\n",
        "                    if isinstance(shap_values, list):\n",
        "                        shap_map = np.mean(np.abs(shap_values[0][0]), axis=-1)\n",
        "                    else:\n",
        "                        shap_map = np.mean(np.abs(shap_values[0]), axis=-1)\n",
        "\n",
        "                    class_shap_values[class_name].append(shap_map)\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image {img_idx}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        # Calculate average SHAP values per class\n",
        "        avg_shap_values = {\n",
        "            class_name: np.mean(values, axis=0) if values else None\n",
        "            for class_name, values in class_shap_values.items()\n",
        "        }\n",
        "\n",
        "        # Create contrastivity matrix\n",
        "        n_classes = len(class_names)\n",
        "        contrast_matrix = np.zeros((n_classes, n_classes))\n",
        "\n",
        "        for i, class1 in enumerate(class_names):\n",
        "            for j, class2 in enumerate(class_names):\n",
        "                if i != j and avg_shap_values[class1] is not None and avg_shap_values[class2] is not None:\n",
        "                    # Calculate contrast score (normalized absolute difference)\n",
        "                    diff = avg_shap_values[class1] - avg_shap_values[class2]\n",
        "                    #contrast_score = np.mean(np.abs(diff)) / (np.mean(np.abs(avg_shap_values[class1])) + np.mean(np.abs(avg_shap_values[class2])) + 1e-6)\n",
        "                    contrast_score = np.mean(np.abs(diff))  # Remove the normalization\n",
        "                    contrast_matrix[i, j] = contrast_score\n",
        "\n",
        "        # Print formatted table\n",
        "        print(f\"\\nClass-wise Contrastivity Scores for {model_name}\")\n",
        "        print(\"=\" * 50)\n",
        "        print(f\"{'':15}\", end=\"\")\n",
        "        for class_name in class_names:\n",
        "            print(f\"{class_name:12}\", end=\"\")\n",
        "        print()\n",
        "\n",
        "        for i, class1 in enumerate(class_names):\n",
        "            print(f\"{class1:15}\", end=\"\")\n",
        "            for j, class2 in enumerate(class_names):\n",
        "                if i == j:\n",
        "                    print(f\"{'-':12}\", end=\"\")\n",
        "                else:\n",
        "                    print(f\"{contrast_matrix[i,j]:12.3f}\", end=\"\")\n",
        "            print()\n",
        "\n",
        "        class_contrasts[model_name] = contrast_matrix\n",
        "\n",
        "    return class_contrasts"
      ],
      "metadata": {
        "id": "Prx8e7jJ5AqM"
      },
      "id": "Prx8e7jJ5AqM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_dict = {\n",
        "    'base': base_model,\n",
        "    'trivial_aug': ta_model,\n",
        "    'gans': gan_model,\n",
        "    'combined': combined_model\n",
        "}\n",
        "\n",
        "\n",
        "class_names = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
        "\n",
        "contrast_matrices = calculate_class_contrast_matrix(\n",
        "    models_dict=models_dict,\n",
        "    test_loader_norm=test_loader_norm,\n",
        "    class_names=class_names,\n",
        "    num_samples=10  # Adjust based on your needs\n",
        ")"
      ],
      "metadata": {
        "id": "UoSQaFrl5YGF",
        "collapsed": true
      },
      "id": "UoSQaFrl5YGF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "contrastivity heatmaps using SHAP:"
      ],
      "metadata": {
        "id": "iOq7KAkKIOYF"
      },
      "id": "iOq7KAkKIOYF"
    },
    {
      "cell_type": "code",
      "source": [
        "def create_enhanced_heatmaps(models_dict, test_loader_norm, test_loader, class_names, num_samples=20):\n",
        "    \"\"\"\n",
        "    Create enhanced SHAP feature importance heatmaps with better visibility\n",
        "    \"\"\"\n",
        "    # Get all normalized images and labels\n",
        "    all_images_norm = []\n",
        "    all_labels = []\n",
        "\n",
        "    for images, labels in test_loader_norm:\n",
        "        all_images_norm.append(images)\n",
        "        all_labels.append(labels)\n",
        "\n",
        "    all_images_norm = torch.cat(all_images_norm, dim=0)\n",
        "    all_labels = torch.cat(all_labels, dim=0).cpu().numpy()\n",
        "\n",
        "    # Create figure with more space between subplots\n",
        "    fig = plt.figure(figsize=(24, 20))\n",
        "    gs = GridSpec(4, 4, figure=fig, hspace=0.3, wspace=0.3)\n",
        "    fig.suptitle('Enhanced Class-Specific SHAP Feature Importance Maps', fontsize=16, y=0.95)\n",
        "\n",
        "    # Store all SHAP values to normalize across all plots\n",
        "    all_shap_values = []\n",
        "\n",
        "    # First pass to collect all SHAP values\n",
        "    for model_name, model in models_dict.items():\n",
        "        print(f\"\\nCollecting SHAP values for model: {model_name}\")\n",
        "        model.eval()\n",
        "\n",
        "        for class_idx, class_name in enumerate(class_names):\n",
        "            class_indices = np.where(all_labels == class_idx)[0]\n",
        "            selected_indices = np.random.choice(class_indices,\n",
        "                                             size=min(num_samples, len(class_indices)),\n",
        "                                             replace=False)\n",
        "\n",
        "            avg_shap_values = None\n",
        "            count = 0\n",
        "\n",
        "            for img_idx in selected_indices:\n",
        "                image = all_images_norm[img_idx].unsqueeze(0).to(next(model.parameters()).device)\n",
        "\n",
        "                try:\n",
        "                    background = torch.zeros_like(image)\n",
        "                    explainer = shap.GradientExplainer(model, background)\n",
        "                    shap_values = explainer.shap_values(image)\n",
        "\n",
        "                    if isinstance(shap_values, list):\n",
        "                        shap_map = np.mean(np.abs(shap_values[class_idx][0]), axis=0)\n",
        "                    else:\n",
        "                        shap_map = np.mean(np.abs(shap_values[0]), axis=0)\n",
        "\n",
        "                    if avg_shap_values is None:\n",
        "                        avg_shap_values = shap_map\n",
        "                    else:\n",
        "                        avg_shap_values += shap_map\n",
        "                    count += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image {img_idx}: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if count > 0:\n",
        "                avg_shap_values /= count\n",
        "                all_shap_values.append(avg_shap_values)\n",
        "\n",
        "    # Get global min and max for consistent scaling\n",
        "    global_max = max(map(np.max, all_shap_values))\n",
        "\n",
        "    # Second pass to create plots with consistent scaling\n",
        "    shap_idx = 0\n",
        "    for model_idx, (model_name, model) in enumerate(models_dict.items()):\n",
        "        for class_idx, class_name in enumerate(class_names):\n",
        "            ax = fig.add_subplot(gs[model_idx, class_idx])\n",
        "\n",
        "            # Use imshow with enhanced settings\n",
        "            im = ax.imshow(all_shap_values[shap_idx],\n",
        "                         cmap='viridis',  # Changed to viridis for better visibility\n",
        "                         vmin=0,\n",
        "                         vmax=global_max)\n",
        "\n",
        "            ax.set_title(f'{model_name}\\n{class_name}', fontsize=12, pad=10)\n",
        "            ax.axis('off')\n",
        "\n",
        "            # Add colorbar with scientific notation\n",
        "            plt.colorbar(im, ax=ax, label='SHAP value', format='%.2e')\n",
        "            shap_idx += 1\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    return fig"
      ],
      "metadata": {
        "id": "QJBMkif-IN08"
      },
      "id": "QJBMkif-IN08",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fig = create_enhanced_heatmaps(models_dict, test_loader_norm, test_loader, class_names)\n",
        "plt.show() # 20 samples"
      ],
      "metadata": {
        "id": "rJVMYBIKX5Me",
        "collapsed": true
      },
      "id": "rJVMYBIKX5Me",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_shap_values(shap_values, class_idx=None):\n",
        "    \"\"\"\n",
        "    Process SHAP values properly for RGB images\n",
        "    \"\"\"\n",
        "    if isinstance(shap_values, list):\n",
        "        if class_idx is not None:\n",
        "            shap_map = shap_values[class_idx][0]  # Get specific class\n",
        "        else:\n",
        "            shap_map = shap_values[0][0]  # Get first class if no specific class\n",
        "    else:\n",
        "        shap_map = shap_values[0]\n",
        "\n",
        "    # Average across channels if dealing with RGB\n",
        "    if shap_map.shape[-1] == 3:  # RGB image\n",
        "        return np.mean(np.abs(shap_map), axis=-1)\n",
        "    return np.mean(np.abs(shap_map), axis=0)\n",
        "\n",
        "def create_enhanced_contrast_heatmaps(models_dict, test_loader_norm, class_names, num_samples=20):\n",
        "    \"\"\"\n",
        "    Create enhanced SHAP feature importance heatmaps using class contrasts\n",
        "    \"\"\"\n",
        "    # Collect samples per class\n",
        "    class_samples = {class_name: {'images': [], 'count': 0} for class_name in class_names}\n",
        "\n",
        "    print(\"Collecting samples...\")\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader_norm:\n",
        "            for img, label in zip(images, labels):\n",
        "                class_name = class_names[label.item()]\n",
        "                if class_samples[class_name]['count'] < num_samples:\n",
        "                    class_samples[class_name]['images'].append(img)\n",
        "                    class_samples[class_name]['count'] += 1\n",
        "\n",
        "            if all(samples['count'] >= num_samples for samples in class_samples.values()):\n",
        "                break\n",
        "\n",
        "    # Calculate number of comparisons\n",
        "    num_comparisons = len(list(itertools.combinations(class_names, 2)))\n",
        "    num_models = len(models_dict)\n",
        "\n",
        "    # Create figure\n",
        "    fig = plt.figure(figsize=(20, 5 * num_models))\n",
        "    gs = GridSpec(num_models, num_comparisons, figure=fig, hspace=0.4, wspace=0.3)\n",
        "    fig.suptitle('Class Contrast SHAP Feature Importance Maps', fontsize=16, y=0.95)\n",
        "\n",
        "    for model_idx, (model_name, model) in enumerate(models_dict.items()):\n",
        "        print(f\"\\nProcessing model: {model_name}\")\n",
        "        model.eval()\n",
        "\n",
        "        # Calculate average SHAP values for each class\n",
        "        class_avg_shap = {}\n",
        "\n",
        "        for class_name in class_names:\n",
        "            print(f\"Processing class: {class_name}\")\n",
        "            if not class_samples[class_name]['images']:\n",
        "                print(f\"Warning: No samples for class {class_name}\")\n",
        "                continue\n",
        "\n",
        "            avg_shap_values = None\n",
        "            count = 0\n",
        "\n",
        "            for image in class_samples[class_name]['images']:\n",
        "                try:\n",
        "                    device = next(model.parameters()).device\n",
        "                    image = image.to(device).unsqueeze(0)\n",
        "\n",
        "                    background = torch.zeros_like(image)\n",
        "                    explainer = shap.GradientExplainer(model, background)\n",
        "                    shap_values = explainer.shap_values(image)\n",
        "\n",
        "                    # Process SHAP values\n",
        "                    shap_map = process_shap_values(shap_values)\n",
        "\n",
        "                    if avg_shap_values is None:\n",
        "                        avg_shap_values = shap_map\n",
        "                    else:\n",
        "                        avg_shap_values += shap_map\n",
        "                    count += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if count > 0:\n",
        "                avg_shap_values /= count\n",
        "                class_avg_shap[class_name] = avg_shap_values\n",
        "\n",
        "        # Create contrast visualizations\n",
        "        for comp_idx, (class1, class2) in enumerate(itertools.combinations(class_names, 2)):\n",
        "            if class1 not in class_avg_shap or class2 not in class_avg_shap:\n",
        "                print(f\"Skipping {class1} vs {class2} - missing values\")\n",
        "                continue\n",
        "\n",
        "            # Calculate contrast\n",
        "            contrast = class_avg_shap[class1] - class_avg_shap[class2]\n",
        "\n",
        "            # Normalize contrast to [-1, 1]\n",
        "            vmax = np.max(np.abs(contrast))\n",
        "            if vmax < 1e-6:\n",
        "                print(f\"Warning: Very small contrast between {class1} and {class2}\")\n",
        "                continue\n",
        "\n",
        "            contrast_normalized = contrast / (vmax + 1e-7)\n",
        "\n",
        "            # Create subplot\n",
        "            ax = fig.add_subplot(gs[model_idx, comp_idx])\n",
        "            im = ax.imshow(contrast_normalized,\n",
        "                          cmap='RdBu_r',\n",
        "                          vmin=-1,\n",
        "                          vmax=1)\n",
        "\n",
        "            ax.set_title(f'{model_name}\\n{class1} vs {class2}', fontsize=10)\n",
        "            plt.colorbar(im, ax=ax, format='%.2e')\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    return fig\n",
        "\n",
        "def visualize_single_class_features(models_dict, test_loader_norm, class_names, num_samples=20):\n",
        "    \"\"\"\n",
        "    Create individual class feature importance heatmaps\n",
        "    \"\"\"\n",
        "    num_models = len(models_dict)\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    fig = plt.figure(figsize=(4 * num_classes, 4 * num_models))\n",
        "    gs = GridSpec(num_models, num_classes, figure=fig, hspace=0.3, wspace=0.3)\n",
        "    fig.suptitle('Single Class SHAP Feature Importance Maps', fontsize=16, y=0.95)\n",
        "\n",
        "    # Collect samples per class\n",
        "    class_samples = {class_name: {'images': [], 'count': 0} for class_name in class_names}\n",
        "\n",
        "    print(\"Collecting samples...\")\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader_norm:\n",
        "            for img, label in zip(images, labels):\n",
        "                class_name = class_names[label.item()]\n",
        "                if class_samples[class_name]['count'] < num_samples:\n",
        "                    class_samples[class_name]['images'].append(img)\n",
        "                    class_samples[class_name]['count'] += 1\n",
        "\n",
        "            if all(samples['count'] >= num_samples for samples in class_samples.values()):\n",
        "                break\n",
        "\n",
        "    all_shap_values = []\n",
        "\n",
        "    for model_idx, (model_name, model) in enumerate(models_dict.items()):\n",
        "        print(f\"\\nProcessing model: {model_name}\")\n",
        "        model.eval()\n",
        "\n",
        "        for class_idx, class_name in enumerate(class_names):\n",
        "            if not class_samples[class_name]['images']:\n",
        "                print(f\"Warning: No samples for class {class_name}\")\n",
        "                continue\n",
        "\n",
        "            avg_shap_values = None\n",
        "            count = 0\n",
        "\n",
        "            for image in class_samples[class_name]['images']:\n",
        "                try:\n",
        "                    device = next(model.parameters()).device\n",
        "                    image = image.to(device).unsqueeze(0)\n",
        "\n",
        "                    background = torch.zeros_like(image)\n",
        "                    explainer = shap.GradientExplainer(model, background)\n",
        "                    shap_values = explainer.shap_values(image)\n",
        "\n",
        "                    # Process SHAP values\n",
        "                    shap_map = process_shap_values(shap_values, class_idx)\n",
        "\n",
        "                    if avg_shap_values is None:\n",
        "                        avg_shap_values = shap_map\n",
        "                    else:\n",
        "                        avg_shap_values += shap_map\n",
        "                    count += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if count > 0:\n",
        "                avg_shap_values /= count\n",
        "                all_shap_values.append(avg_shap_values)\n",
        "\n",
        "                # Normalize values to [0, 1]\n",
        "                normalized_values = avg_shap_values / (np.max(avg_shap_values) + 1e-7)\n",
        "\n",
        "                ax = fig.add_subplot(gs[model_idx, class_idx])\n",
        "                im = ax.imshow(normalized_values,\n",
        "                             cmap='viridis',\n",
        "                             vmin=0,\n",
        "                             vmax=1)\n",
        "\n",
        "                ax.set_title(f'{model_name}\\n{class_name}', fontsize=10)\n",
        "                plt.colorbar(im, ax=ax, format='%.2e')\n",
        "                ax.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    return fig"
      ],
      "metadata": {
        "id": "SC69wvIxYpX2"
      },
      "id": "SC69wvIxYpX2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contrast_fig = create_enhanced_contrast_heatmaps(\n",
        "    models_dict,\n",
        "    test_loader_norm,\n",
        "    class_names,\n",
        "    num_samples=20\n",
        ")\n",
        "contrast_fig.savefig('contrast_heatmaps.png', bbox_inches='tight', dpi=300)\n",
        "\n",
        "single_class_fig = visualize_single_class_features(\n",
        "    models_dict,\n",
        "    test_loader_norm,\n",
        "    class_names,\n",
        "    num_samples=5\n",
        ")\n",
        "single_class_fig.savefig('single_class_heatmaps.png', bbox_inches='tight', dpi=300)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "z_xuIiC8Ys-c"
      },
      "id": "z_xuIiC8Ys-c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_shap_values(shap_values, class_idx=None):\n",
        "    \"\"\"\n",
        "    Process SHAP values properly for RGB images\n",
        "    \"\"\"\n",
        "    # Print shape information for debugging\n",
        "    print(\"SHAP values type:\", type(shap_values))\n",
        "    if isinstance(shap_values, list):\n",
        "        print(\"SHAP values[0] shape:\", shap_values[0][0].shape)\n",
        "    else:\n",
        "        print(\"SHAP values shape:\", shap_values.shape)\n",
        "\n",
        "    if isinstance(shap_values, list):\n",
        "        if class_idx is not None:\n",
        "            # For classification, take the specified class\n",
        "            shap_map = shap_values[class_idx][0]\n",
        "        else:\n",
        "            # If no class specified, take the first class\n",
        "            shap_map = shap_values[0][0]\n",
        "    else:\n",
        "        shap_map = shap_values[0]\n",
        "\n",
        "    print(\"Shape before reduction:\", shap_map.shape)\n",
        "\n",
        "    # Handle different possible shapes\n",
        "    if len(shap_map.shape) == 4:  # (C, H, W, 3) or similar\n",
        "        # Average across channels and RGB if present\n",
        "        shap_map = np.mean(np.abs(shap_map), axis=(0, -1))\n",
        "    elif len(shap_map.shape) == 3:  # (C, H, W) or (H, W, 3)\n",
        "        if shap_map.shape[-1] == 3:  # RGB\n",
        "            shap_map = np.mean(np.abs(shap_map), axis=-1)\n",
        "        else:  # Channels first\n",
        "            shap_map = np.mean(np.abs(shap_map), axis=0)\n",
        "\n",
        "    print(\"Shape after reduction:\", shap_map.shape)\n",
        "    return shap_map\n",
        "\n",
        "def create_enhanced_contrast_heatmaps(models_dict, test_loader_norm, class_names, num_samples=20):\n",
        "    \"\"\"\n",
        "    Create enhanced SHAP feature importance heatmaps using class contrasts\n",
        "    \"\"\"\n",
        "    # Collect samples per class\n",
        "    class_samples = {class_name: {'images': [], 'count': 0} for class_name in class_names}\n",
        "\n",
        "    print(\"Collecting samples...\")\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader_norm:\n",
        "            for img, label in zip(images, labels):\n",
        "                class_name = class_names[label.item()]\n",
        "                if class_samples[class_name]['count'] < num_samples:\n",
        "                    class_samples[class_name]['images'].append(img)\n",
        "                    class_samples[class_name]['count'] += 1\n",
        "\n",
        "            if all(samples['count'] >= num_samples for samples in class_samples.values()):\n",
        "                break\n",
        "\n",
        "    # Calculate number of comparisons\n",
        "    num_comparisons = len(list(itertools.combinations(class_names, 2)))\n",
        "    num_models = len(models_dict)\n",
        "\n",
        "    # Create figure\n",
        "    fig = plt.figure(figsize=(20, 5 * num_models))\n",
        "    gs = GridSpec(num_models, num_comparisons, figure=fig, hspace=0.4, wspace=0.3)\n",
        "    fig.suptitle('Class Contrast SHAP Feature Importance Maps', fontsize=16, y=0.95)\n",
        "\n",
        "    for model_idx, (model_name, model) in enumerate(models_dict.items()):\n",
        "        print(f\"\\nProcessing model: {model_name}\")\n",
        "        model.eval()\n",
        "\n",
        "        # Calculate average SHAP values for each class\n",
        "        class_avg_shap = {}\n",
        "\n",
        "        for class_name in class_names:\n",
        "            print(f\"Processing class: {class_name}\")\n",
        "            if not class_samples[class_name]['images']:\n",
        "                print(f\"Warning: No samples for class {class_name}\")\n",
        "                continue\n",
        "\n",
        "            avg_shap_values = None\n",
        "            count = 0\n",
        "\n",
        "            for image in class_samples[class_name]['images']:\n",
        "                try:\n",
        "                    device = next(model.parameters()).device\n",
        "                    image = image.to(device).unsqueeze(0)\n",
        "\n",
        "                    background = torch.zeros_like(image)\n",
        "                    explainer = shap.GradientExplainer(model, background)\n",
        "                    shap_values = explainer.shap_values(image)\n",
        "\n",
        "                    # Process SHAP values\n",
        "                    shap_map = process_shap_values(shap_values)\n",
        "\n",
        "                    if avg_shap_values is None:\n",
        "                        avg_shap_values = shap_map\n",
        "                    else:\n",
        "                        avg_shap_values += shap_map\n",
        "                    count += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if count > 0:\n",
        "                avg_shap_values /= count\n",
        "                class_avg_shap[class_name] = avg_shap_values\n",
        "\n",
        "        # Create contrast visualizations\n",
        "        for comp_idx, (class1, class2) in enumerate(itertools.combinations(class_names, 2)):\n",
        "            if class1 not in class_avg_shap or class2 not in class_avg_shap:\n",
        "                print(f\"Skipping {class1} vs {class2} - missing values\")\n",
        "                continue\n",
        "\n",
        "            # Calculate contrast\n",
        "            contrast = class_avg_shap[class1] - class_avg_shap[class2]\n",
        "\n",
        "            # Normalize contrast to [-1, 1]\n",
        "            vmax = np.max(np.abs(contrast))\n",
        "            if vmax < 1e-6:\n",
        "                print(f\"Warning: Very small contrast between {class1} and {class2}\")\n",
        "                continue\n",
        "\n",
        "            contrast_normalized = contrast / (vmax + 1e-7)\n",
        "\n",
        "            # Create subplot\n",
        "            ax = fig.add_subplot(gs[model_idx, comp_idx])\n",
        "            # Ensure contrast is 2D and properly normalized\n",
        "            if len(contrast_normalized.shape) > 2:\n",
        "                print(f\"Warning: Unexpected shape {contrast_normalized.shape}, reducing dimensions\")\n",
        "                contrast_normalized = np.mean(contrast_normalized, axis=tuple(range(len(contrast_normalized.shape)-2)))\n",
        "\n",
        "            im = ax.imshow(contrast_normalized,\n",
        "                          cmap='RdBu_r',\n",
        "                          vmin=-1,\n",
        "                          vmax=1,\n",
        "                          interpolation='nearest')\n",
        "\n",
        "            ax.set_title(f'{model_name}\\n{class1} vs {class2}', fontsize=10)\n",
        "            plt.colorbar(im, ax=ax, format='%.2e')\n",
        "            ax.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    return fig\n",
        "\n",
        "def visualize_single_class_features(models_dict, test_loader_norm, class_names, num_samples=20):\n",
        "    \"\"\"\n",
        "    Create individual class feature importance heatmaps\n",
        "    \"\"\"\n",
        "    num_models = len(models_dict)\n",
        "    num_classes = len(class_names)\n",
        "\n",
        "    fig = plt.figure(figsize=(4 * num_classes, 4 * num_models))\n",
        "    gs = GridSpec(num_models, num_classes, figure=fig, hspace=0.3, wspace=0.3)\n",
        "    fig.suptitle('Single Class SHAP Feature Importance Maps', fontsize=16, y=0.95)\n",
        "\n",
        "    # Collect samples per class\n",
        "    class_samples = {class_name: {'images': [], 'count': 0} for class_name in class_names}\n",
        "\n",
        "    print(\"Collecting samples...\")\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader_norm:\n",
        "            for img, label in zip(images, labels):\n",
        "                class_name = class_names[label.item()]\n",
        "                if class_samples[class_name]['count'] < num_samples:\n",
        "                    class_samples[class_name]['images'].append(img)\n",
        "                    class_samples[class_name]['count'] += 1\n",
        "\n",
        "            if all(samples['count'] >= num_samples for samples in class_samples.values()):\n",
        "                break\n",
        "\n",
        "    all_shap_values = []\n",
        "\n",
        "    for model_idx, (model_name, model) in enumerate(models_dict.items()):\n",
        "        print(f\"\\nProcessing model: {model_name}\")\n",
        "        model.eval()\n",
        "\n",
        "        for class_idx, class_name in enumerate(class_names):\n",
        "            if not class_samples[class_name]['images']:\n",
        "                print(f\"Warning: No samples for class {class_name}\")\n",
        "                continue\n",
        "\n",
        "            avg_shap_values = None\n",
        "            count = 0\n",
        "\n",
        "            for image in class_samples[class_name]['images']:\n",
        "                try:\n",
        "                    device = next(model.parameters()).device\n",
        "                    image = image.to(device).unsqueeze(0)\n",
        "\n",
        "                    background = torch.zeros_like(image)\n",
        "                    explainer = shap.GradientExplainer(model, background)\n",
        "                    shap_values = explainer.shap_values(image)\n",
        "\n",
        "                    # Process SHAP values\n",
        "                    shap_map = process_shap_values(shap_values, class_idx)\n",
        "\n",
        "                    if avg_shap_values is None:\n",
        "                        avg_shap_values = shap_map\n",
        "                    else:\n",
        "                        avg_shap_values += shap_map\n",
        "                    count += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing image: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "            if count > 0:\n",
        "                avg_shap_values /= count\n",
        "                all_shap_values.append(avg_shap_values)\n",
        "\n",
        "                # Normalize values to [0, 1]\n",
        "                normalized_values = avg_shap_values / (np.max(avg_shap_values) + 1e-7)\n",
        "\n",
        "                ax = fig.add_subplot(gs[model_idx, class_idx])\n",
        "                # Ensure values are 2D\n",
        "                if len(normalized_values.shape) > 2:\n",
        "                    print(f\"Warning: Unexpected shape {normalized_values.shape}, reducing dimensions\")\n",
        "                    normalized_values = np.mean(normalized_values, axis=tuple(range(len(normalized_values.shape)-2)))\n",
        "\n",
        "                im = ax.imshow(normalized_values,\n",
        "                             cmap='viridis',\n",
        "                             vmin=0,\n",
        "                             vmax=1,\n",
        "                             interpolation='nearest')\n",
        "\n",
        "                ax.set_title(f'{model_name}\\n{class_name}', fontsize=10)\n",
        "                plt.colorbar(im, ax=ax, format='%.2e')\n",
        "                ax.axis('off')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    return fig"
      ],
      "metadata": {
        "id": "sYOUPqU1h7sY"
      },
      "id": "sYOUPqU1h7sY",
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contrast_fig = create_enhanced_contrast_heatmaps(\n",
        "    models_dict,\n",
        "    test_loader_norm,\n",
        "    class_names,\n",
        "    num_samples=3  # Start with fewer samples\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "t6uyeGLRk4VN"
      },
      "id": "t6uyeGLRk4VN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "contrast_fig = create_enhanced_contrast_heatmaps(\n",
        "    models_dict,\n",
        "    test_loader_norm,\n",
        "    class_names,\n",
        "    num_samples=20\n",
        ")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "q0ZOgAwHlCmp"
      },
      "id": "q0ZOgAwHlCmp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "test regardless imaging planes:"
      ],
      "metadata": {
        "id": "8qTwIa9T5VHS"
      },
      "id": "8qTwIa9T5VHS"
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_shap_distributions(models_dict, test_loader_norm, class_names, num_samples=20):\n",
        "    \"\"\"\n",
        "    Analyze SHAP value distributions and sparsity across classes and models.\n",
        "    Focus on view-independent metrics.\n",
        "    \"\"\"\n",
        "    def calculate_sparsity_metrics(shap_values):\n",
        "        \"\"\"Calculate various sparsity metrics for SHAP values\"\"\"\n",
        "        # Flatten SHAP values\n",
        "        flat_shap = np.abs(shap_values).flatten()\n",
        "\n",
        "        # Calculate metrics\n",
        "        gini = 1 - np.sum((flat_shap / np.sum(flat_shap)) ** 2)\n",
        "        top_10_percent = np.sum(np.sort(flat_shap)[-int(len(flat_shap)*0.1):]) / np.sum(flat_shap)\n",
        "        sparsity = np.mean(flat_shap < np.max(flat_shap) * 0.1)  # Fraction of small values\n",
        "\n",
        "        return {\n",
        "            'gini': gini,\n",
        "            'top_10_percent': top_10_percent,\n",
        "            'sparsity': sparsity\n",
        "        }\n",
        "\n",
        "    def calculate_class_contrast_metrics(shap_values_dict):\n",
        "        \"\"\"Calculate contrast metrics between classes\"\"\"\n",
        "        contrasts = {}\n",
        "        for c1 in class_names:\n",
        "            for c2 in class_names:\n",
        "                if c1 < c2:\n",
        "                    # Calculate KL divergence between SHAP distributions\n",
        "                    s1 = np.abs(shap_values_dict[c1]).flatten()\n",
        "                    s2 = np.abs(shap_values_dict[c2]).flatten()\n",
        "\n",
        "                    # Normalize to probability distributions\n",
        "                    s1 = s1 / np.sum(s1)\n",
        "                    s2 = s2 / np.sum(s2)\n",
        "\n",
        "                    # Add small epsilon to avoid division by zero\n",
        "                    epsilon = 1e-10\n",
        "                    s1 += epsilon\n",
        "                    s2 += epsilon\n",
        "                    s1 /= np.sum(s1)\n",
        "                    s2 /= np.sum(s2)\n",
        "\n",
        "                    # Calculate symmetric KL divergence\n",
        "                    kl_div = (entropy(s1, s2) + entropy(s2, s1)) / 2\n",
        "\n",
        "                    # Calculate Wasserstein distance (approximation using sorted values)\n",
        "                    s1_sorted = np.sort(s1)\n",
        "                    s2_sorted = np.sort(s2)\n",
        "                    wasserstein = np.mean(np.abs(s1_sorted - s2_sorted))\n",
        "\n",
        "                    contrasts[f\"{c1}_vs_{c2}\"] = {\n",
        "                        'kl_divergence': kl_div,\n",
        "                        'wasserstein': wasserstein\n",
        "                    }\n",
        "        return contrasts\n",
        "\n",
        "    # Store results for each model\n",
        "    results = {}\n",
        "\n",
        "    # Process each model\n",
        "    for model_name, model in models_dict.items():\n",
        "        print(f\"\\nProcessing model: {model_name}\")\n",
        "        model.eval()\n",
        "\n",
        "        # Collect SHAP values per class\n",
        "        class_shap_values = {class_name: [] for class_name in class_names}\n",
        "\n",
        "        # Process samples\n",
        "        sample_count = {class_name: 0 for class_name in class_names}\n",
        "\n",
        "        for images, labels in test_loader_norm:\n",
        "            if all(count >= num_samples for count in sample_count.values()):\n",
        "                break\n",
        "\n",
        "            for img, label in zip(images, labels):\n",
        "                class_name = class_names[label.item()]\n",
        "                if sample_count[class_name] >= num_samples:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Get SHAP values\n",
        "                    background = torch.zeros_like(img.unsqueeze(0))\n",
        "                    explainer = shap.GradientExplainer(model, background)\n",
        "                    shap_values = explainer.shap_values(img.unsqueeze(0))\n",
        "\n",
        "                    # Process SHAP values\n",
        "                    if isinstance(shap_values, list):\n",
        "                        processed_shap = np.mean(np.abs(shap_values[0][0]), axis=0)\n",
        "                    else:\n",
        "                        processed_shap = np.mean(np.abs(shap_values[0]), axis=0)\n",
        "\n",
        "                    class_shap_values[class_name].append(processed_shap)\n",
        "                    sample_count[class_name] += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing sample: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        # Calculate metrics\n",
        "        model_results = {\n",
        "            'sparsity': {},\n",
        "            'contrasts': {},\n",
        "            'distributions': {}\n",
        "        }\n",
        "\n",
        "        # Average SHAP values per class\n",
        "        avg_shap_values = {}\n",
        "        for class_name, values in class_shap_values.items():\n",
        "            if values:\n",
        "                avg_shap_values[class_name] = np.mean(values, axis=0)\n",
        "                # Calculate sparsity metrics\n",
        "                model_results['sparsity'][class_name] = calculate_sparsity_metrics(avg_shap_values[class_name])\n",
        "\n",
        "        # Calculate contrast metrics\n",
        "        model_results['contrasts'] = calculate_class_contrast_metrics(avg_shap_values)\n",
        "\n",
        "        # Store distribution information\n",
        "        for class_name, values in class_shap_values.items():\n",
        "            if values:\n",
        "                flat_values = np.concatenate([v.flatten() for v in values])\n",
        "                model_results['distributions'][class_name] = {\n",
        "                    'mean': np.mean(flat_values),\n",
        "                    'std': np.std(flat_values),\n",
        "                    'percentiles': np.percentile(flat_values, [25, 50, 75])\n",
        "                }\n",
        "\n",
        "        results[model_name] = model_results\n",
        "\n",
        "    return results\n",
        "\n",
        "def visualize_metrics(results):\n",
        "    \"\"\"\n",
        "    Create visualizations for the sparsity and contrast metrics\n",
        "    \"\"\"\n",
        "    num_models = len(results)\n",
        "    fig = plt.figure(figsize=(15, 5 * num_models))\n",
        "    gs = GridSpec(num_models, 3, figure=fig)\n",
        "\n",
        "    for i, (model_name, model_results) in enumerate(results.items()):\n",
        "        # Plot sparsity metrics\n",
        "        ax1 = fig.add_subplot(gs[i, 0])\n",
        "        sparsity_data = []\n",
        "        labels = []\n",
        "        metrics = []\n",
        "        for class_name, metrics_dict in model_results['sparsity'].items():\n",
        "            for metric_name, value in metrics_dict.items():\n",
        "                sparsity_data.append(value)\n",
        "                labels.append(class_name)\n",
        "                metrics.append(metric_name)\n",
        "\n",
        "        ax1.bar(range(len(sparsity_data)), sparsity_data)\n",
        "        ax1.set_xticks(range(len(sparsity_data)))\n",
        "        ax1.set_xticklabels([f\"{l}\\n{m}\" for l, m in zip(labels, metrics)], rotation=45)\n",
        "        ax1.set_title(f\"{model_name} - Sparsity Metrics\")\n",
        "\n",
        "        # Plot contrast metrics\n",
        "        ax2 = fig.add_subplot(gs[i, 1])\n",
        "        contrast_data = []\n",
        "        contrast_labels = []\n",
        "        for pair, metrics in model_results['contrasts'].items():\n",
        "            contrast_data.append(metrics['kl_divergence'])\n",
        "            contrast_labels.append(pair)\n",
        "\n",
        "        ax2.bar(range(len(contrast_data)), contrast_data)\n",
        "        ax2.set_xticks(range(len(contrast_data)))\n",
        "        ax2.set_xticklabels(contrast_labels, rotation=45)\n",
        "        ax2.set_title(f\"{model_name} - KL Divergence Between Classes\")\n",
        "\n",
        "        # Plot distribution metrics\n",
        "        ax3 = fig.add_subplot(gs[i, 2])\n",
        "        for class_name, dist_metrics in model_results['distributions'].items():\n",
        "            ax3.boxplot([dist_metrics['percentiles']], positions=[list(model_results['distributions'].keys()).index(class_name)],\n",
        "                       labels=[class_name])\n",
        "        ax3.set_title(f\"{model_name} - SHAP Value Distributions\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig\n"
      ],
      "metadata": {
        "id": "hJuArZHk5aQO",
        "collapsed": true
      },
      "id": "hJuArZHk5aQO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_dict = {\n",
        "    'base': base_model,\n",
        "    'trivial_aug': ta_model,\n",
        "    'gans': gan_model,\n",
        "    'combined': combined_model\n",
        "}\n",
        "\n",
        "\n",
        "class_names = ['Glioma', 'Meningioma', 'No Tumor', 'Pituitary']\n",
        "\n",
        "results = analyze_shap_distributions(\n",
        "    models_dict,\n",
        "    test_loader_norm,\n",
        "    class_names,\n",
        "    num_samples=20\n",
        ")\n",
        "\n",
        "fig = visualize_metrics(results)\n",
        "plt.savefig('shap_analysis.png', bbox_inches='tight', dpi=300)"
      ],
      "metadata": {
        "id": "IiPmyqpMGmsT"
      },
      "id": "IiPmyqpMGmsT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_shap_summary_plots(models_dict, test_loader_norm, class_names, num_samples=20):\n",
        "    \"\"\"\n",
        "    Create SHAP summary visualizations that don't rely on spatial interpretations\n",
        "    \"\"\"\n",
        "    def process_shap_values(shap_values):\n",
        "        if isinstance(shap_values, list):\n",
        "            return shap_values[0][0]  # Get first class\n",
        "        return shap_values[0]\n",
        "\n",
        "    def calculate_feature_stats(shap_map):\n",
        "        # Calculate various statistics that don't depend on spatial location\n",
        "        abs_shap = np.abs(shap_map)\n",
        "        return {\n",
        "            'mean_importance': np.mean(abs_shap),\n",
        "            'max_importance': np.max(abs_shap),\n",
        "            'percentile_95': np.percentile(abs_shap, 95),\n",
        "            'sparsity': np.mean(abs_shap < np.max(abs_shap) * 0.1)\n",
        "        }\n",
        "\n",
        "    results = {}\n",
        "    for model_name, model in models_dict.items():\n",
        "        print(f\"\\nProcessing model: {model_name}\")\n",
        "        model.eval()\n",
        "\n",
        "        # Collect samples and SHAP values per class\n",
        "        class_stats = {class_name: [] for class_name in class_names}\n",
        "        shap_magnitudes = {class_name: [] for class_name in class_names}\n",
        "\n",
        "        sample_count = {class_name: 0 for class_name in class_names}\n",
        "\n",
        "        for images, labels in test_loader_norm:\n",
        "            if all(count >= num_samples for count in sample_count.values()):\n",
        "                break\n",
        "\n",
        "            for img, label in zip(images, labels):\n",
        "                class_name = class_names[label.item()]\n",
        "                if sample_count[class_name] >= num_samples:\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    # Calculate SHAP values\n",
        "                    background = torch.zeros_like(img.unsqueeze(0))\n",
        "                    explainer = shap.GradientExplainer(model, background)\n",
        "                    shap_values = explainer.shap_values(img.unsqueeze(0))\n",
        "\n",
        "                    # Process SHAP values\n",
        "                    shap_map = process_shap_values(shap_values)\n",
        "                    stats = calculate_feature_stats(shap_map)\n",
        "                    class_stats[class_name].append(stats)\n",
        "\n",
        "                    # Store magnitudes for distribution plots\n",
        "                    shap_magnitudes[class_name].extend(np.abs(shap_map).flatten())\n",
        "\n",
        "                    sample_count[class_name] += 1\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing sample: {str(e)}\")\n",
        "                    continue\n",
        "\n",
        "        results[model_name] = {\n",
        "            'stats': class_stats,\n",
        "            'magnitudes': shap_magnitudes\n",
        "        }\n",
        "\n",
        "    # Create visualizations\n",
        "    num_models = len(results)\n",
        "    fig = plt.figure(figsize=(15, 5 * num_models))\n",
        "    gs = GridSpec(num_models, 3, figure=fig)\n",
        "\n",
        "    for model_idx, (model_name, model_results) in enumerate(results.items()):\n",
        "        # 1. Feature Importance Violin Plot\n",
        "        ax1 = fig.add_subplot(gs[model_idx, 0])\n",
        "        violin_data = []\n",
        "        violin_positions = []\n",
        "        violin_labels = []\n",
        "\n",
        "        for i, (class_name, magnitudes) in enumerate(model_results['magnitudes'].items()):\n",
        "            violin_data.append(magnitudes)\n",
        "            violin_positions.append(i)\n",
        "            violin_labels.append(class_name)\n",
        "\n",
        "        ax1.violinplot(violin_data, positions=violin_positions)\n",
        "        ax1.set_xticks(violin_positions)\n",
        "        ax1.set_xticklabels(violin_labels, rotation=45)\n",
        "        ax1.set_title(f\"{model_name} - Feature Importance Distributions\")\n",
        "\n",
        "        # 2. Summary Statistics\n",
        "        ax2 = fig.add_subplot(gs[model_idx, 1])\n",
        "        for stat_name in ['mean_importance', 'max_importance', 'percentile_95']:\n",
        "            stat_values = []\n",
        "            for class_name in class_names:\n",
        "                class_stat = np.mean([stats[stat_name]\n",
        "                                    for stats in model_results['stats'][class_name]])\n",
        "                stat_values.append(class_stat)\n",
        "\n",
        "            ax2.plot(class_names, stat_values,\n",
        "                    marker='o',\n",
        "                    label=stat_name.replace('_', ' ').title())\n",
        "\n",
        "        ax2.set_xticklabels(class_names, rotation=45)\n",
        "        ax2.legend()\n",
        "        ax2.set_title(f\"{model_name} - Summary Statistics\")\n",
        "\n",
        "        # 3. Sparsity Comparison\n",
        "        ax3 = fig.add_subplot(gs[model_idx, 2])\n",
        "        sparsity_values = []\n",
        "        for class_name in class_names:\n",
        "            sparsity = np.mean([stats['sparsity']\n",
        "                              for stats in model_results['stats'][class_name]])\n",
        "            sparsity_values.append(sparsity)\n",
        "\n",
        "        ax3.bar(class_names, sparsity_values)\n",
        "        ax3.set_xticklabels(class_names, rotation=45)\n",
        "        ax3.set_title(f\"{model_name} - Feature Sparsity\")\n",
        "        ax3.set_ylim(0, 1)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    return fig"
      ],
      "metadata": {
        "id": "rJOPO4itd52K"
      },
      "id": "rJOPO4itd52K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summary_fig = create_shap_summary_plots(\n",
        "    models_dict,\n",
        "    test_loader_norm,\n",
        "    class_names,\n",
        "    num_samples=20\n",
        ")\n",
        "summary_fig.savefig('shap_summary.png', bbox_inches='tight', dpi=300)"
      ],
      "metadata": {
        "id": "ggvPZuepd-ET",
        "collapsed": true
      },
      "id": "ggvPZuepd-ET",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "3yihh0wgGRcp"
      ]
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}